\chapter{state of the art}
\label{ch:state}

% Současný stav řešené problematiky, přehled klasických i aktuálních výsledků a jejich porovnání, analýza problematiky vedoucí k vymezení oblasti zájmu budoucí disertační práce, 8 - 12 stran.

\section{Preliminaries}
\label{sec:preliminaries}

This section introduces the notation used in the rest of the thesis proposal.
The presented notation is mainly based on the configurable software
verification technique \cite{Beyer2007, Beyer2018, Beyer2018b} and Cousot's
notation used to describe abstract interpretation \cite{Cousot2012}.

\subsection{Programs, Control-Flow Graph, States}

We restrict the presentation to a simplified version of \llvmir programs
\cite{Lattner04}. The \llvmir is byte-code intermediate representation of the
open-source \llvm compiler infrastructure.  The latter is a well-main\-tained,
well-documented, and continuously improving framework. Because of its faithful
correspondence with the executable program and robust infrastructure, \llvm
presents a sweet spot where to conduct program analysis.

An elementary unit of \llvm program is an \emph{instruction}. We recognize a usual set of binary
instructions (arithmetic, relational operations), memory access operations
(\texttt{load}, \texttt{store}), heap operations (\texttt{malloc},
\texttt{free}), stack allocation (\texttt{alloca}), data flow operations
(branching instructions \texttt{br}, calls, return instruction \texttt{ret}).
Since \llvmir is a typed language, it also provides conversion instructions
among integers, floats and pointers. We admit scalar types (floats, integers),
pointer types, and aggregate types (arrays, structures).  For detailed syntax
and semantics description we refer the interested reader to \autoref{ch:llvm}.

The entire program in \llvm consists of global variables and functions.  A
function is a directed graph of basic blocks. A basic block is a sequence of
non-terminal instructions terminated by a terminal instruction.\sidenote{A
terminal instruction are for example branching or return instruction.}

For a more accessible representation of an \llvm program, we will use a
\emph{control flow automaton} (CFA), which is a directed graph with \llvm
instructions attached to its edges (see \autoref{fig:cfa}).

\begin{definition}
    Given a set of instructions $\mathcal{I}$, a control flow automaton
    $\mathcal{A}$ is a tuple $(\mathcal{L}, l_{\textit{init}}, \mathcal{G})$,
    where $\mathcal{L}$ is a set of program locations, $l_{\textit{init}} \in
    \mathcal{L}$ is an initial location that represents a program entry point
    and $\mathcal{G} \subseteq (\mathcal{L} \times \mathcal{I} \times
    \mathcal{L})$ is set of edges between program locations, each labeled with
    an instruction that is executed when the control flows along the edge.
\end{definition}

The goal of the analyses that we will investigate throughout this chapter is to
determine for each program location a set of possible concrete states described
by some abstract representation. A \emph{concrete program state} is determined
by values of the program registers $\mathcal{R}$ and memory content. The number of \llvm
registers is unbounded, and they represent results of individual instructions
in the static single assignment (\ssa) form~\cite{Cytron1991}. The memory
consists of stack-allocated variables $\mathcal{S}$ created by \texttt{alloca}
instruction, heap memory content $\mathcal{H}$ allocated via \texttt{malloc}, and
global variables $\mathcal{V}_{\textit{G}}$; the only permitted access to memory is
through dedicated instructions (\texttt{store} and \texttt{load}).  For
simplicity, we expect that the memory is strongly typed; hence, we always know
what type is assigned to what memory location.
We denote a set of variables as $\mathcal{V} = \mathcal{S} \cup \mathcal{H}
\cup \mathcal{V}_{\textit{G}}$ and all possible concrete values as
$\mathcal{C}$.  Program values contains all possible values of registers and
variables as well as memory addresses $\mathcal{C}_{\textit{addr}} \subseteq
\mathcal{C}$.

\begin{definition}
    A \textbf{\emph{concrete state}} of \llvm program $\sigma = (\rho, \epsilon, \mu)$, where:
\begin{itemize}
    \item $\rho \colon \mathcal{R} \to \mathcal{C}$ is a value map that assigns each program
        register a~concrete value,
    \item $\epsilon \colon \mathcal{V} \to \mathcal{C}_{\textit{addr}}$ is a
        program environment that maps variables to their addresses, and
    \item $\mu \colon \mathcal{C}_{\textit{addr}} \to \mathcal{C}$ is a
        partial function that maps addresses into values.
\end{itemize}
\end{definition}

\noindent
We denote the set of all concrete states as $\Sigma$.  A~set of admissible
states at some location $l \in \mathcal{L}$ is called a \emph{context}
$\mathcal{C}_l \subseteq \mathcal{P}(\Sigma)$.

The semantics of instruction $i \in \mathcal{I}$ is defined by \emph{strongest
post condition} $\textsf{SP}_{i}(\cdot)$, i.e. it is executed on a particular program
state and accordingly updates values of program values and memory content.

A set of all edges $\mathcal{G}$ induces a transition relation $\rightarrow$ on
the set of states $\Sigma$, such that for each $g \in \mathcal{G}$ there is
$\xrightarrow{g} \: \subseteq \Sigma \times \{g\} \times \Sigma$. The
transition relation is a union over all edges $\rightarrow = \bigcup_{g
\in\mathcal{G}} \xrightarrow{g}$.

A \emph{path} $\pi$ is a sequence of edges $\langle
\rightarrow_1, \rightarrow_2, \dots, \rightarrow_n \rangle$ such that all edges
form a continuous path in the transition system induced by $\mathcal{G}$.  A path
is called a \emph{program path} if it starts from initial program location
$l_{\textit{init}}$. The semantics of path $\pi$ is defined by iterative
application of $SP_{i}( \cdot )$ for each instruction in path $\pi$. Given
initial constraints $\varphi$ on program state, the strongest postcondition of
path $\pi$ is $\textsf{SP}_{\pi} =
\textsf{SP}_{i_n}(\dots,\textsf{SP}_{i_1}(\varphi),\dots)$. A path $\pi$ is
called \emph{feasible} if $\textsf{SP}_{\pi}(\textit{true})$ is satisfiable and
\emph{infeasible} otherwise \cite{Beyer2018b}.  In the abstract execution, the
transition system does not contain particular states but whole contexts that
abstract sets of states. A set of possible transition from a context contains
all feasible transitions from concrete states included in the context.
A \emph{verification task} for  CFA $\mathcal{A} = (\mathcal{L},
l_{\textit{init}}, \mathcal{G})$ is to show that error location
$l_{\textit{err}} \in \mathcal{L}$ is unreachable in $\mathcal{A}$, or to find
feasible error path.

\begin{figure}

\begin{minipage}[t]{0.5\textwidth}
\begin{minted}[linenos]{llvm}
define i32 @main() {
entry:
  %x = alloca i32
  store i32 0 to i32* %x
  br label %loop
loop:
  %v = load i32* %x
  %a = add i32 %v, 1
  store i32 %a to i32* %x
  %b = icmp ult i32 %a, 5
  br %b, label %loop, label %end
end:
  ret i32 0
}
\end{minted}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\strut\vspace*{-\baselineskip}\newline\centering
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [loc, below = of l6] (l7) {$l_7$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};

    \node [loc, below = of l8] (l9) {$l_9$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);

\end{tikzpicture}
\end{minipage}

\caption{An example of \llvm program (left) and its CFA (right).  The program
allocates on stack a 32-bit integer variable and stores its address to register
\llvmint{|\var{\%x}|} (registers in \llvm are traditionally prefixed with \%,
and global objects as functions by @) Subsequently in the loop, program
increments value of the variable while it is smaller than 5. Syntax of \llvm
instructions is described in \autoref{ch:llvm}.}
\label{fig:cfa}
\end{figure}

\section{Abstract interpretation}
\label{sec:abstraction}

The original idea of abstract interpretation dates back to the late 70s, first
summarized by Patrick and Radhia Cousot \cite{Cousot1977}.
They describe \emph{abstract interpretation} as a theory of abstraction and
constructive approximation of the mathematical structures used in the formal
description of programming languages or verification of undecidable program
properties~\cite{Cousot2012}.

For us, the goal of abstract interpretation is to assign each program location
a context in a given domain, i.e., given a set of program states, the
abstraction represents properties of these states by abstract state. Program
contexts are computed by solving a system of fixpoint equations generated from
a~transition function \cite{Cousot1977}. Such computation simulates the
execution of a program in an abstract domain. As a result, each abstract
context represents possible properties (invariants about states) of variables
at a location after the execution of the program with arbitrary input. The
states\sidenote{State of computation comprise of all intermediate contexts
assigned to program locations.} of the computation forms a complete lattice:
$(S, \sqcap, \sqcup, \sqsubseteq, \top, \bot)$, where $S$ is a set of states,
$\sqcup$, $\sqcap$ are meet and join operators, $\sqsubseteq$ ordering of a
lattice, $\top$ is the greatest element and $\bot$ it the least element of the
lattice. Throughout the rest of the thesis proposal, we will assume familiarity
with the basic notions of lattice theory \cite{Birkhoff1940} and abstract
interpretation~\cite{Cousot1977}.

Given the concrete contexts from previous definitions, the \emph{concrete
semantic domain} is a complete lattice $(\mathcal{P}(\Sigma), \cap, \cup,
\subseteq, \Sigma, \emptyset)$. Even though this technique gives us a tool to
compute all possible states that a program might reach, it is, unfortunately,
uncomputable. To mitigate the problem of computability, we employ an abstract
domain instead of a concrete one, i.e., we need to drop some information about
program variables. For instance, to drop all the information about each program
variable except whether it is positive, negative or zero. This can be achieved
by replacement of values domain we are computing on $\mathsf{C}$ by an abstract
domain $\domainm{sign} = \{\bot, -, 0, +, \top\}$, where $\bot$ represents an
undefined value (variable without assigned value), $+$,$-$ and $0$ denote a
\begin{marginfigure}%
    \centering
    \begin{tikzpicture}[node distance=1em]
    \node [] (t) {$\top$};
    \node [below = of t] (0) {$0$};
    \node [left = of 0] (m) {$-$};
    \node [right = of 0] (p) {$+$};
    \node [below = of 0] (b) {$\bot$};
    \draw [thin] (t) -- (m) -- (b);
    \draw [thin] (t) -- (0) -- (b);
    \draw [thin] (t) -- (p) -- (b);
    \end{tikzpicture}
    \caption{$\mathsf{A}_{\textit{sign}}$ domain lattice.}
    \label{fig:signd}%
\end{marginfigure}%
sign of variable and $\top$ is for an arbitrary value (i.e., variable can be
either negative, positive or zero).

\begin{figure}[t]
\begin{adjustwidth}{-\oddsidemargin-1in}{-\rightmargin}
\centering
\begin{minipage}[t]{0.3\textwidth +  0.3\marginparsep + 0.3\marginparwidth}
\strut\vspace*{-\baselineskip}\newline\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};
    \node [lab, right = 0cm of l1] (e1) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [lab, right = 0cm of l2] (e2) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};
    \node [lab, right = 0cm of l3] (e3) {$(0, \bot, \bot, \bot)$};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [lab, right = 0cm of l4] (e4) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [lab, right = 0cm of l5] (e5) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [lab, right = 0cm of l6] (e6) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l6] (l7) {$l_7$};
    \node [lab, right = 0cm of l7] (e7) {$(\bot, \bot, \bot, \bot)$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};
    \node [lab, right = 0cm of l8] (e8) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l8] (l9) {$l_9$};
    \node [lab, right = 0cm of l9] (e9) {$(\bot, \bot, \bot, \bot)$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);
\end{tikzpicture}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth +  0.3\marginparsep + 0.3\marginparwidth}
\strut\vspace*{-\baselineskip}\newline\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};
    \node [lab, right = 0cm of l1] (e1) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [lab, right = 0cm of l2] (e2) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};
    \node [lab, right = 0cm of l3] (e3) {$(0, \bot, \bot, \bot)$};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [lab, right = 0cm of l4] (e4) {$(0, 0, \bot, \bot)$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [lab, right = 0cm of l5] (e5) {$(0, 0, +, \bot)$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [lab, right = 0cm of l6] (e6) {$(+, 0, +, \bot)$};
    \node [loc, below = of l6] (l7) {$l_7$};
    \node [lab, right = 0cm of l7] (e7) {$(+, 0, +, \top)$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};
    \node [lab, right = 0cm of l8] (e8) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l8] (l9) {$l_9$};
    \node [lab, right = 0cm of l9] (e9) {$(\bot, \bot, \bot, \bot)$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);
\end{tikzpicture}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth +  0.3\marginparsep + 0.3\marginparwidth}
\strut\vspace*{-\baselineskip}\newline\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};
    \node [lab, right = 0cm of l1] (e1) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [lab, right = 0cm of l2] (e2) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};
    \node [lab, right = 0cm of l3] (e3) {$(\top, \top, \top, \top)$};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [lab, right = 0cm of l4] (e4) {$(\top, \top, \top, \top)$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [lab, right = 0cm of l5] (e5) {$(\top, \top, \top, \top)$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [lab, right = 0cm of l6] (e6) {$(\top, \top, \top, \top)$};
    \node [loc, below = of l6] (l7) {$l_7$};
    \node [lab, right = 0cm of l7] (e7) {$(\top, \top, \top, \top)$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};
    \node [lab, right = 0cm of l8] (e8) {$(\top, \top, \top, \top)$};

    \node [loc, below = of l8] (l9) {$l_9$};
    \node [lab, right = 0cm of l9] (e9) {$(\top, \top, \top, \top)$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);
\end{tikzpicture}
}
\end{minipage}
    \caption{\domain{sign} fixpoint iteration for values of stack variables and
    registers (\llvmint{|\var{\%x}, \var{\%v}, \var{\%a}, \var{\%b}|}). A~tuple
    at each location represents an abstract evaluation of the registers in this
    order.  The presented states of iteration are after first iteration
    (\textbf{left}), after 5 iterations (\textbf{middle}), reached fixpoint
    (\textbf{right}).}
    \label{fig:signcomp}%
    \end{adjustwidth}
\end{figure}%

We say that $\mathsf{A}$ is an abstraction of $\mathsf{C}$ because it carries
less information about program variables. One requirement on $\mathsf{A}$ is
that its values form a complete lattice (see \autoref{fig:signd}). Moreover, a
well-defined domain is required to have an image for each concrete value, and
 is over approximating abstraction. Such properties of domains are described by
 \textbf{\emph{Galois connection}}.

 \begin{definition}
 For the concrete lattice $(C, \subseteq)$ and abstract lattice $(A,
 \sqsubseteq)$ a~galois connection is $(\alpha, \gamma)$, where
 $\alpha \colon C \to A$, $\gamma \colon A \to C$ are monotonic functions
 such that:

\bigskip
\begin{center}
\begin{minipage}{0.45\textwidth}
\centering
    \emph{Abstraction \\ over-approximates}
\begin{tikzpicture}
    \node [domainbox, thick, color = apple, label=\color{apple}$C$] (c) {};
    \node [context, below = 1em of c.north] (cs) {};
    \node [below = 0.5em of cs.north] (co) {\textbullet$c$};
    \node [domainbox, thick, color = vivid, right = of c, label=\color{vivid}$A$] (a) {};
    \node [context, below = 1em of a.north] (as) {};

    \draw [->, >=stealth] (co) to [bend left=40] node[above] {$\alpha$} (as);
    \draw [->, >=stealth] (as) to [bend left=40] node[below] {$\gamma$} (cs);
\end{tikzpicture}
    \[
        \forall c \in C.\: c \subseteq \gamma( \alpha( c ) )
    \]
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
    \emph{Abstraction after concretisation yields no imprecision}
\begin{tikzpicture}
    \node [domainbox, thick, color = apple, label=\color{apple}$C$] (c) {};
    \node [context, below = 1em of c.north] (cs) {};
    \node [domainbox, thick, color = vivid, right = of c, label=\color{vivid}$A$] (a) {};
    \node [below = 0.5em of as.north] (ao) {$a$};
    \node [context, below = 1em of a.north] (as) {};

    \draw [->, >=stealth] (as) to [bend right=40] node[above] {$\gamma$} (cs);
    \draw [->, >=stealth] (cs) to [bend right=40] node[below] {$\alpha$} (ao);
\end{tikzpicture}
    \[
        \forall a \in A.\: \alpha( \gamma( a ) ) \sqsubseteq a
    \]
\end{minipage}
\end{center}

\noindent
In abstract interpretation, $C$ and $A$ are called \emph{concrete} respectivelly
\emph{abstract} domain and they are assumed to be complete lattices. We call
$\alpha$ the \emph{abstraction} map and $\gamma$ the \emph{concretization} map.

\end{definition}

\prule
\bigskip

\noindent
Given an abstract domain and abstract operations\sidenote{In our case, abstract
semantics of \llvm instructions.} we perform a fixpoint iteration\sidenote{For
detailed algorithm see \cite{Cousot1977}} and by Knaster-Tarski theorem
\cite{Tarski1955} we are guaranteed to obtain a solution, see
\autoref{fig:signcomp}.

Even though our example program in \autoref{fig:signcomp} is quite trivial, the
sign domain \domain{sign} was not able to preserve any interesting properties,
and all resulting contexts contains either $\top$ or $\bot$.  Sign domain belongs to
the class of simple domains that reason only about single property of variables --
also known as \emph{property domains}. Another famous example of a property
domain is a \textbf{\emph{parity domain}}, which captures whether the
variable is odd or even. The parity domain is just a special case of
a~\textbf{\emph{congruence domain}} \cite{Granger1989, Granger1991}.
\begin{marginfigure}%
    \centering
\resizebox{\textwidth}{!}{
    \begin{tikzpicture}[node distance=1em]
    \node [] (t) {$\top$};
    \node [below = 3em of t] (0) {$0$};
    \node [left = of 0] (m1) {$-1$};
    \node [left = of m1] (m2) {$-2$};
    \node [left = of m2] (m) {$\dots$};
    \node [right = of 0] (p1) {$1$};
    \node [right = of p1] (p2) {$2$};
    \node [right = of p2] (p) {$\dots$};
    \node [below = 3em of 0] (b) {$\bot$};
    \draw [thin] (t) -- (m) -- (b);
    \draw [thin] (t) -- (m2) -- (b);
    \draw [thin] (t) -- (m1) -- (b);
    \draw [thin] (t) -- (0) -- (b);
    \draw [thin] (t) -- (p1) -- (b);
    \draw [thin] (t) -- (p2) -- (b);
    \draw [thin] (t) -- (p) -- (b);
    \end{tikzpicture}
}
    \caption{\domain{cp} domain lattice.}
    \label{fig:cp}%
\end{marginfigure}%
A~\textbf{\emph{constant propagation domain}} \domain{cp}
(see~\autoref{fig:cp}) is another example of property domain used in
optimization techniques \cite{Kildall1973}.

Probably the simplest of domains is a~\textbf{\emph{single value}} domain
\domain{U}. The single value domain is useful if we want to omit a variable
from the model of the program. The two-value domain that contains two possible
elements ($\top, \bot$) is also called the \textbf{\emph{definedness domain}}
\domain{D}. Because with $\top$, we can denote that value is defined,
respectively undefined with $\bot$. Despite its simplicity, it can be useful to
detect reachability of program locations.

\begin{marginfigure}%
    \centering
\resizebox{\textwidth}{!}{
    \begin{tikzpicture}[node distance=1em]
    \node [] (t) {$\top$};
    \node [below = 6em of t] (0) {$0$};
    \node [left = of 0] (m1) {$-1$};
    \node [left = of m1] (m2) {$-2$};
    \node [above = 3em of m2] (mm) {$-$};
    \node [left = of m2] (m) {$\dots$};
    \node [right = of 0] (p1) {$1$};
    \node [right = of p1] (p2) {$2$};
    \node [right = of p2] (p) {$\dots$};
    \node [above = 3em of p2] (pp) {$+$};
    \node [below = 3em of 0] (b) {$\bot$};
        \draw [thin] (t) -- (mm) -- (m)  -- (b);
        \draw [thin] (t) -- (mm) -- (m2) -- (b);
        \draw [thin] (t) -- (mm) -- (m1) -- (b);
    \draw [thin] (t) -- (0) -- (b);
        \draw [thin] (t) -- (pp)-- (p1) -- (b);
        \draw [thin] (t) -- (pp)-- (p2) -- (b);
        \draw [thin] (t) -- (pp) -- (p) -- (b);
    \end{tikzpicture}
}
    \caption{\domain{cs} is a joint sign (\domain{sign}) and constant propagation (\domain{cp}) domain.}
    \label{fig:cpsign}%
\end{marginfigure}%

Furthermore, it is possible to combine multiple domains to
reason about multiple properties either by keeping each property separately
or~by reduced product of domains (see \autoref{fig:cpsign}), which is more
precise and more efficient than multiple separate interpretations
\cite{Cousot2011b}.  For further information see domain refinement in
\autoref{sec:domainrefinement}.

The basic building block of these domains is \emph{basis} that expresses how to
abstract one program variable or register and operations on it.  Thanks to the
galois connection the rest of interpretation can be derived automatically as a
product domain for all variables \cite{Mine2004Thesis}.

Given the state of a program, each of the program variables and registers
requires a various level of abstraction, as well as different abstract domains.
Depending on what values we abstract, we distinguish an abstraction of numerical
values, memory content, and also program pointers. Consequently, the entire
abstraction of program state is formed by a product domain of all used
abstractions. Moreover, some parts of the state might be omitted from the
abstraction entirely and represented in the concrete domain.

Since our goal is to formulate a unified model for all of these abstractions,
in the following sections, we will bring the state-of-the-art abstract
domains to our \llvm model so that we can build upon them. Furthermore, we
detect missing components of these approaches so that they can be used in the
abstraction of \llvm. Based on the purpose of the abstract domains, in \llvm we
recognize three types of abstractions:
\begin{enumerate}
    \item \emph{scalar abstraction} conducts analysis only on basic types:
        integers or floats,
    \item \emph{aggregate abstraction} takes care of abstract representation of
        memory objects like homogeneous arrays, strings, or heterogeneous
        structures, and
    \item \emph{pointer abstraction} handle representation of non-deterministic
        pointers that can contain an arbitrary address or just random offset
        from a concrete address.
\end{enumerate}

\section{Scalar abstract domains}
\label{sec:domains}

Scalar abstract domains take care of representation of basic \llvm values as
integers or floating-points. The traditional scalar domains are also known in
the literature as \emph{numerical abstract domains} defined by Cousot
in~\cite{Cousot1977}. In Cousot's approach, the abstraction is designed only
for programs without memory and recursion; hence, abstract only program
variables and performs context-insensitive analysis. We will call this
restricted model as \emph{Cousot's program model}. Since \llvm does not
directly correspond to this model, we need to adjust Cousot's model to our
state description. In \llvm for an abstract domain \domain{}, the numerical
abstraction extends the state definition so that registers can hold abstract
values. These program registers can be handled as Cousot's variables. However,
the problem arises on the boundaries with memory.  The simplest solution would
be to lower abstract values to concrete domain with concretization function
$\gamma$, but we would lose any advantage of abstraction and lose its
precision.  Therefore we need to extend \llvm program memory to be able to hold
abstract scalar values.

Cousots's model can also be extended to context-sensitive analysis, either by
abstracting call-stack for interprocedural analysis~\cite{Jeannet2004,
Pascal2011}, or summarization of functions~\cite{Boutonnet2019}. These
techniques are extended to handle recursion in multiple works
\cite{Cousot1977Recursion, CousotCousot01}.  Alternatively, the interprocedural
analysis can be achieved by a dynamic approach. The dynamic execution allows
the analysis to preserve context-sensitivity because it preserves call-stack as
in the state of the program. Since we aim to perform abstract execution of
\llvm dynamically, we will obtain the interprocedural analysis as a consequence
of the execution.

\begin{definition}
    For abstract scalar domain $\mathcal{A}$, an abstract program state is
    defined as $(\rho, \epsilon, \mu)$, where:

    \begin{itemize}
        \itemsep0em
        \item $\rho \colon \mathcal{R} \to \mathcal{C} \cup \mathcal{A}$ allows to store abstract values into registers,
    \item $\epsilon \colon \mathcal{V} \to \mathcal{C}_{\textit{addr}}$ is a
        concrete program environment, and
    \item $\mu \colon \mathcal{C}_{\textit{addr}} \to \mathcal{C} \cup
        \mathcal{A}$ permits to keep abstract values in program memory.
    \end{itemize}
\end{definition}

\begin{marginfigure}
\strut\vspace*{-\baselineskip}\newline\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};
    \node [lab, right = 0cm of l1] (e1) {$\emptyset$};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [lab, right = 0cm of l2] (e2) {$\emptyset$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};
    \node [lab, right = 0cm of l3] (e3) {$[0,4]$};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [lab, right = 0cm of l4] (e4) {$[0,4]$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [lab, right = 0cm of l5] (e5) {$[0,4]$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [lab, right = 0cm of l6] (e6) {$[0,4]$};
    \node [loc, below = of l6] (l7) {$l_7$};
    \node [lab, right = 0cm of l7] (e7) {$[1,5]$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};
    \node [lab, right = 0cm of l8] (e8) {$[5,5]$};

    \node [loc, below = of l8] (l9) {$l_9$};
    \node [lab, right = 0cm of l9] (e9) {$[5,5]$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);
\end{tikzpicture}
}
    \caption{Result of interval abstraction for content of stack variable
    \llvmint{|\var{\%x}|} after fixpoint iteration.}
    \label{fig:interval}
\end{marginfigure}

Since the precision of an abstract domain highly depends on the program
structure, many abstract domains were designed to track various program
properties. The first domain presented by Cousot~\cite{Cousot1977} to
achieve more precise results was an \textbf{\emph{interval domain}} \domain{I}.
In the interval abstraction, a set of values is represented by its minimal and
maximal value (see \autoref{fig:interval}).  Because in some cases, it is not
possible to determine the boundary values, the interval domain utilizes
infinities to denote arbitrary bound -- we represent arbitrary value
$\top_{\textit{I}}$ by $[-\infty, \infty]$ and undetermined value
$\bot_{\textit{I}}$ by an empty interval~$\emptyset$.

Unfortunately, with the infinite lattice of the interval
domain,\sidenote{The interval domain does not meet ascending chain condition.} the
abstract interpretation will obtain a possible infinite or impractically long
executions (e.g., when the interpretation can not determine the bound of a loop
it can increment the boundary of an interval indefinitely never reaching the
fixpoint). Therefore, abstract interpreters employ \textbf{\emph{widening}} and
\textbf{\emph{narrowing}} techniques to accelerate convergence \cite{Cousot1992a,
Cortesi2011}.  The idea of wi\-de\-ning is to overshoot the least fixpoint
after few unsuccessful iterations of the interpretation and subsequently by
narrowing to refine the over-approximated solution. By different implementation
of widening and narrowing, abstract interpreters employ different strategies to
achieve convergence -- for example widening with thresholds \cite{Blanchet2003,
Lakhdar2011}, delayed widening, parma widening \cite{Bagnara2008} or abstract
acceleration \cite{Gonnord2006, Feautrier2010}.

\begin{marginfigure}
\begin{minted}[linenos]{c}
int i = 10;
int v = 0;
while (i >= 0) {
    i = i - 1;
    if (random())
        v = v + 1;
}
\end{minted}
    \caption{Program that requires a relational invariant.}
    \label{fig:relationalc}
\end{marginfigure}

An alternative approach to tackle infinite interpretation that does not require
any extrapolation operator is a \textbf{\emph{policy iteration}}
\cite{Costan2005, Gaubert2007, Gawlitza2007, Gawlitza2007b, Gawlitza2011}. The
idea of policies is to compute fixpoint solution of a sequence of simpler
semantic equations, such that the least fixpoint is reached after a finite
number of iterations.  The sequence of policies defines a strategy to approach
the fixpoint either from above or below. Policies are formed from a
decomposition of original abstraction.

The drawback of the domains presented uptil now is that they are not able to reason about
relational properties between variables. Therefore, we address them as
\emph{non-relational} abstract domains.  However, many verification tasks
require to take variables relations into account.  For example, to show that $v \leq 11$
at line 7 in program of \autoref{fig:relationalc}, we need to prove a
relational loop invariant $v + i
\leq 10$.

%\add{other applications of relational domains: analysis of programs with
%symbolic parameters, modular analysis of procedures, inference of non-uniform
%non-numerical invariants (pointer analysis)}

\subsection{Relational abstract domains}

The simplest of the relational abstract domains is a \emph{linear equality
domain} that captures information about affine relationships among program
variables \cite{Karr1976}. In this domain, an abstract variable is represented
by the affine subspace of program state space. The problem with the affine
domain is in its nonunique minimal form. Therefore, we can not easily determine
whether two abstract values are equal. This problem is solved by combining
the affine domain with the congruence domain \cite{Granger1991}.

Another relational domain is a \emph{polyhedron abstract domain} which is
inspired by the theory of linear programming and optimization \cite{Cousot1978, Venet2012}
and provides a unique representation. However, the manipulation with polyhedron
abstract values involves costly computation of a simplex algorithm
\cite{Schrijver1986}. Nowadays a~polyhedron abstract domain is still
extensively studied: Singh et~al. present an abstraction of polyhedron by
bounding boxes to simplify abstract operations in \cite{Singh2017};
alternatively the combination with interval domain to reason about interval
linear properties is examined in \cite{Chen2009}.

\subsection{Weakly relational abstract domains}

In the middle between relational and non-relational abstract domains stand
weakly relational abstract domains. These domains aim to enhance the precision
of non-relational domains and reduce the cost of abstraction of relational
domains by keeping only a reduced number of relations.

The notable representant of weakly-relational domains is an \emph{octagon
abstract domain} \cite{Mine2006}. This domain efficiently enables the representation
of invariants of form $\pm x \pm y \leq c$, where $x$ and $y$ are program
variables and $c$ is a constant.  Moreover, the advantage of the octagon domain
is in its representation by potential graphs and difference-bound matrices
\cite{Larsen1997}, which allows a unique minimal representation of abstract
values.\sidenote{Unique representation is highly demanded in the program
verification because it allows fast equality comparisons of abstract states.}
The octagon domain was further optimized by bucketing of octagons
\cite{Blanchet2003, Venet2004} and modified to extend captured properties
\cite{Claris2004, Mine2004}. Further restriction provides a \emph{zone domain}
\cite{Mine2001} which reasons only about invariants of form $x - y \leq c$ and
$\pm x \leq c$.

A less precise and lightweight relational domain of \emph{pentagons} was introduced in~\cite{Logozzo2010} with applicability on validation of array manipulating
byte-code. The pentagon domain captures properties of the form $x \in [a,b]
\wedge x < y$.

A recent abstract domain utilizes \emph{parallelotopes} to encode any linear
constraint as polyhedra do, while it preserves the efficiency of weakly
relational abstract domains \cite{Amato2017}.

A disadvantageous aspect of relational abstract domains is their complicated
representation and therefore computationally less efficient abstract
operations. In addition to maintaining more properties about variables, the
representation also needs to assure uniqueness of the representation. Hence,
the operations need to incorporate some sort of normalization (e.g., difference
bound matrices in the octagon domain \cite{Mine2006}, or generators of affine
space in the linear equality domain \cite{Karr1976}).

Except picking of a suitable abstraction, a precision of abstract methods can
be improved by symbolic methods. Miné presents in his work \cite{Mine2006b} a
set of lightweight simplifications (linearization and symbolic constant
propagation) which are used to transform numerical expressions in order to
increase the precision of the analysis. Likewise, Chang takes use of congruence
closure extensions~\cite{Chang2005}.

\subsubsection{Predicate Abstraction}

In program verification, predicate abstraction \cite{Graf1997, Bjorner1997} is
a prominent technique used to handle infinite state-spaces (see
\autoref{tab:svcomp}). Elements of the abstract domain are abstract predicates
over program variables. The interpretation of predicates is a set of program
states (i.e., the states of abstracted system correspond to truth assignments
to a set of predicates). Because the predicate abstraction is restricted to a
finitary domain, its transition function can be computed by \textsc{smt} solver/theorem
prover~\cite{Cousot2003, Ball2001}. The predicates used in abstraction are
generated in a heuristic manner from the program under test or by a refinement
loop (see \autoref{fig:cegar}) \cite{Flanagan2002}. In contrast to previous
abstract domains, a predicate representation may not just reason about a single
variable but a whole program state (set of states), moreover predicates
inherently keep relational information between variables.

\subsubsection{Domain of uninterpreted functions}

The domain that is of particular interest for us in the following chapters is a
domain of \emph{uninterpreted functions}~\cite{Gange2016}. This domain is targeted
at problems of relational abstract domains -- poor scalability and loss of
tracking of non-linear properties. The idea of the domain is to treat program
instructions as uninterpreted function symbols -- i.e., built expression trees,
where leaves are nullary terms (program variables) and nodes represent n-ary
operations. This domain can be used to augment non-relational abstract domains
with relational information.

\bigskip
\prule
\bigskip

So far, we have presented only domains, which abstract only scalar values.
Overall a \emph{scalar abstract domain} is determined by three primary
components:

\phantomsection
\label{sec:ingredients}
\begin{enumerate}
    \itemsep0em
    \item a poset $(\mathcal{A}, \sqsupseteq)$ with concretization and abstraction function,
    \item effective and sound abstract operators,
    \item and an iteration strategy.
\end{enumerate}

%\bigskip
%\prule
%\bigskip

%\noindent
%In the following section we investigate non-scalar abstract domains, generally
%known as symbolic abstractions. They abstract infinite sets of in/finite
%functions \cite{Cousot14}, graphs and other constructs of programming languages
%like arrays~\cite{Cousot2011p}, trees~\cite{Mauborgne2000}, heaps (non sharing
%\cite{Cousot1977static, Cousot1977rec} or shape analysis~\cite{Reps2002,
%Chang2008}).

\section{Aggregate abstraction}

In a general setting, analyses of programming languages require more
specialized abstractions to reason about complex constructs as aggregates or
memory, than just the scalar ones. In \llvm, we recognize two types of
aggregates: homogenous like arrays or vectors, or heterogeneous structures.
Although it may seem reasonable to represent aggregates as products of scalars
abstract domains, in practice such representation is a cost-inefficient.
Moreover, product representation is restricted only to the exact size of
aggregate, hence does not allow an analysis of arbitrary large objects.
Therefore, many tools utilize various domain specific abstractions to reason
efficiently about aggregates.

In comparison to scalar domains, aggregate domains do not abstract arithmetic
operations, but they behave like memory objects. Therefore they abstract memory
access operations (\texttt{store}/ \texttt{load}) and also allow to compute an
offset to their elements. These offsets might be abstract, but the address of
the aggregates is required to be concrete, i.e., only the content behind the
pointer is abstracted -- this is the key difference to more general heap
abstractions. The aggregate domain might also combine with scalar domains to
represent internal elements, or abstract indices used to represent the
arbitrary large sizes.

\subsection{Array abstraction}

The simplest and heavily used aggregate -- \emph{array} gets a lot of specific
abstract domains for efficient reasoning about array manipulating programs. In
the analysis of arrays, the analyzer needs to be able to discover relationships
among values of array elements, as well as their relationships to scalar
variables and constants located in the program.

Generally, it is problematic to statically reason about arrays, because of
their unbounded nature. The problem is that the array operations do not
consider a particular fixed array size. Instead, the array size typically
depends on the scalar variables. The precise verification of such program
requires being able to reason about relationships between array size, array
content and program variables. Hence, analyses tend to utilize symbolic methods
to represent constraints on arrays~\cite{Gopan2005}.

Depending on the interested properties, we recognize two types of array-related
techniques. The analysis either focus on the reasoning about array bounds and
related out of bounds errors, or on the content of the array~\cite{Halbwachs2008}.

To reason about array content, abstract domains are usually para\-me\-tri\-zable by
numerical abstract domains by which the abstraction represents the internal
elements of an abstract array~\cite{Cousot2011p}. Symbolic methods are also
used to represent or generalize facts about array manipulations -- for example,
\emph{universally quantified abstract domains}, enable to quantify over array
elements with the use of simpler abstract domains~\cite{Gulwani2008}.
Predicate abstraction is also used to describe properties about arrays in a
form of update predicates~\cite{Kovacs2009}.

The symbolic techniques naturally request for a refinement of abstractions.
The refinement of universally quantified abstract domains is presented
in~\cite{Seghir2009}.  As universal quantifiers represent hard task for
decision procedures; the refinement tries to eliminate their occurrences using
ghost variables. Furthermore, the refinement techniques leverage that
predicates have a particular form when they reason about array indices. Other
refinement approaches utilize lazy-abstraction~\cite{Alberti2012} and
interpolants~\cite{Alberti2012b}.

The recent work~\cite{Payet2018} summarizes how weakly-relational domains for
array indices does not scale in the analysis of object-oriented languages.
However, in combination with symbolic expressions, traditional domains can
scale to real-world codebases.

A different approach is via program instrumentation~\cite{Cornish2015} that
lowers program arrays into scalar manipulating code. In this way, we obtain
array-free programs that leverage the fine-tuned numerical analysis. Moreover,
the instrumentation can slice program only for interested array properties.

\subsection{String abstraction}

Even though array abstraction might be used to handle strings in programs, it
is more precise to treat them separately. In C language as well as in \llvm,
strings are represented as null-terminated arrays of characters. Hence, to
detect string related errors, many abstractions extends the array abstractions
to keep record about null characters~\cite{Olliaro2018}.

Since string domains are more restricted, they utilize other formalisms to
represent an abstract set of strings like finite
automata~\cite{Christensen2003} or trie data structures.

The recent work of Constantini~\cite{Costantini2011, Costantini2015} proposes a
unifying approach to string analysis.  In his work, presented domains can be
tuned to a different level of precision and address only specific properties of
strings.


\section{Heap abstraction}

Because heap memory is potentially unbounded and seemingly arbitrary, we cannot
employ the same techniques as with stack and static memory. Another key
difference of memory abstractions to previous techniques is that memory permits
self-referential values, i.e., abstraction needs to manage possibly infinitely
recursive structures. Hence, a general heap abstraction consists of two
components:

\begin{enumerate}
    \item \emph{heap modeling}, which takes care of heap memory representation
        as memory model (i.e., an unbounded set of concrete locations as an
        unbounded set of abstract locations), and
    \item \emph{summarization}, which bounds the memory by merging multiple
        abstract locations into summary locations.
\end{enumerate}

\noindent
In literature, we recognize multiple types of heap models: a \emph{store-based
model}, which represents memory as a graph with symbolic addresses, where edge
$p \rightarrow l$ represents that a pointer $p$ may hold the address of
concrete location $l$ (see \autoref{fig:storebased}); on the contrary, a
\emph{storeless model} abstracts locations in terms of access paths and capture
the structure of relations between heap objects~\cite{Kanvar2016}.

\begin{marginfigure}

\footnotesize

\begin{minted}[linenos,fontsize=\scriptsize]{c}
    Node *list = malloc(Node);
    Node *n = list;
    while (true) {
      n->next = malloc(Node);
      n = x->next;
    }
\end{minted}

Store based model of concrete heap of previous program, where $l_1,
{l_4}_1$ and ${l_4}_2$ denote allocation sites:

\bigskip
\begin{tikzpicture}[node distance=2.2em]
    \node [mem] (l1) {};
    \node [below = 0em of l1] (a1) {$l_1$};
    \node [above = 1em of l1] (p1) {\texttt{list}};

    \node [mem, right = of l1] (l2) {};
    \node [below = 0em of l2] (a2) {${l_4}_1$};
    \node [above = 1em of l2] (p2) {\texttt{n}};

    \node [mem, right = of l2] (l3) {};
    \node [below = 0em of l3] (a3) {${l_4}_2$};
    \node [above = 1em of l3] (p3) {\texttt{n}};

    \node [right = of l3] (l4) {$\dots$};

    \draw [->, >=stealth] (l1) -- node[midway, above] {\scriptsize\texttt{next}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, above] {\scriptsize\texttt{next}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, above] {\scriptsize\texttt{next}} (l4);

    \draw [->, >=stealth] (p1) -- (l1);
    \draw [->, >=stealth] (p2) -- (l2);
    \draw [->, >=stealth] (p3) -- (l3);
\end{tikzpicture}

Allocation site based summarization:

\bigskip
\begin{tikzpicture}[node distance=2.2em]
    \node [mem] (l1) {};
    \node [below = 0em of l1] (a1) {$l_1$};
    \node [above = 1em of l1] (p1) {\texttt{list}};

    \node [mem, right = of l1] (l2) {};
    \node [below = 0em of l2] (a2) {$l_4$};
    \node [above = 1em of l2] (p2) {\texttt{n}};

    \draw [->, >=stealth] (l1) -- node[midway, above] {\scriptsize\texttt{next}} (l2);
%    \draw [->, >=stealth] (l2) -- node[midway, above] {\scriptsize\texttt{next}} (l3);

    \draw [->, >=stealth] (p1) -- (l1);
    \draw [->, >=stealth] (p2) -- (l2);

    \draw [->, >=stealth] (l2) to [out=30,in=-30,looseness=8] node [right] {\scriptsize\texttt{next}} (l2);
\end{tikzpicture}
\caption{Heap summarization example.}
\label{fig:storebased}
\end{marginfigure}

\subsubsection{Pointer abstraction}

In the memory-aware analysis, we may perform a separate abstraction of
pointers. The simplest of them are non-relational pointer abstractions. A
non-relational pointer abstraction is defined in the same way as a~scalar
abstraction; it is given as lattice of abstract pointer values
$A_{\textit{addr}}$; concretization function
$\gamma: A_{\textit{addr}} \rightarrow
\mathcal{P}(\mathcal{C}_{\textit{addr}})$ and abstraction function $\alpha:
\mathcal{P}(\mathcal{C}_{\textit{addr}}) \rightarrow A _{\textit{addr}}$. An
example of simple non-relational pointer abstract domain is a \emph{null
pointer} domain illustrated in \autoref{fig:nulldomain}.

\begin{definition}
    For abstract pointer domain $\mathcal{A}_{\textit{addr}}$, an abstract program state is
    defined as $(\rho, \epsilon, \mu)$, where:

    \begin{itemize}
        \itemsep0em
        \item $\rho \colon \mathcal{R} \to \mathcal{C} \cup
            \mathcal{A}_{\textit{addr}}$ registers may contain abstract
            addresses,
        \item $\epsilon \colon \mathcal{V} \to \mathcal{C}_{\textit{addr}}
            \cup \mathcal{A}_{\textit{addr}}$ environment allows to assign
            objects symbolic addresses, and
        \item $\mu \colon \mathcal{C}_{\textit{addr}} \cup
            \mathcal{A}_{\textit{addr}} \to \mathcal{C} \cup
            \mathcal{A}_{\textit{addr}}$ permits to keep values in abstract
            memory, as well as referencing another abstract addresses.
    \end{itemize}
\end{definition}

\begin{marginfigure}

    \centering
    \begin{tikzpicture}[node distance=1em]
    \node [] (t) {$\top$};
    \node [below = of t] (null) {$\textit{non-null}$};
    \node [below = of null] (b) {$\bot$};
        \draw [thin] (t) -- (null) -- (b);
    \end{tikzpicture}
    \caption{The null pointer domain keeps information whether a pointer might be \emph{null}:
    $\gamma_{\textit{ptr}}(\top) = \mathcal{C}_{\textit{addr}}$ and
    $\gamma_{\textit{ptr}}(\textit{non-null}) = \mathcal{C}_{\textit{addr}} \setminus 0$. }
    \label{fig:nulldomain}
\end{marginfigure}

Non-Relational pointer domains have a disadvantage because of their imprecision
-- as soon as a points-to set of pointers contain several elements, the
abstract interpretation performs imprecise updates. Moreover, they are not able
to express well-structured invariants about memory objects (e.g., lists or trees).

For more precise updates, pointer abstraction leverage summarization
techniques, also known as \emph{shape analysis}. Shape graphs are used to
generalize abstract data structures (e.g., lists or trees). The shape graph can
summarize inductive data structures into simple shapes~\cite{Distefano2006}.
Similarly to scalar analysis, shape analysis can be refined by widening or by
keeping shape relations~\cite{Chang2008}.  Reduced products to shape analysis
was introduced in~\cite{Toubhans2013}.

More recent approaches leverage \textsc{smt} solvers and reduce problems from
separational logic, that is used in shape analysis, to propositional
logics~\cite{Piskac2013, Itzhaky2014, Itzhaky2014b}.  Also, template-based
domains~\cite{Malik2018} delegates semantic reasoning to \textsc{smt} solvers
and concentrates on the design of appropriate shape templates.  In comparison
to traditional predicate analysis where a user needs to supply a set of
predicates, template-based domains use a set of parametrized predicates which
are further gradually refined via \textsc{smt}.  Moreover, the \textsc{smt}
representation enables a straightforward combination with value analysis --
hence allows reasoning about the shape and their contents at the same time.

To reason only about numeric properties of heap manipulating programs, we can
omit a lot of shape analysis. Magill et al. present an
abstraction~\cite{Magill2010} that instruments only numeric abstraction of
shape properties into the program, hence allow cheaper reasoning about
properties like a length of linked-lists or distance between two elements of
the list. Other domains extend the relational numeric analysis of the content
of particular shapes, for example, trees~\cite{Journault2019}.

Many tools specialize in the analysis of a single type of data-structures. To
name a few: Predator~\cite{Dudka2012} is a tool for automated verification of programs with
dynamic linked data structures using a separation logic and shape
graphs; Forester~\cite{Holik2015} leverage tree automata in the combination
with shape analysis to verify programs with various types of lists or
trees; whereas Space Invader~\cite{Yang2008} use shape
analysis in combination with abduction. For an extended survey of heap
abstractions, we refer to~\cite{Kanvar2016}.

\subsection{Verification dedicated abstractions}

In addition to general-purpose domains presented so far, verification of
real-world programs requires many domain-specific abstractions. Since most of
numerical domains work with unbounded numbers, they are not well suited for
sound representation of computer numbers. Therefore, we would like to reflect
the properties of bit-precise arithmetic like overflows in the abstract
domains.  This problem is solved in recent abstract domains that leverage
bit-vector logic to reproduce processor semantics~\cite{Sharma2013,
Sharma2017}. Similar domains were also designed for floating-point
arithmetic~\cite{Putot2004}.  The real-world program analysis also requires to
reflect memory model of the program under test. To describe memory model
properties, analysis usually leverage dedicated predicate
abstraction~\cite{Dan2013}. Further challenges of machine code analysis are
outlined in~\cite{Reps2010}.

More domain-specific abstractions are used to analyze database querying
languages~\cite{Halder2012}; system components like filters~\cite{Feret2004} or
syn\-chro\-nous clocks~\cite{Feret2005}.

\subsection{Domain Refinement}
\label{sec:domainrefinement}

\begin{figure}%
    \centering
    \begin{tikzpicture}[node distance=1em]
        \node [align=center] (con) {$\mathcal{P}(\mathbb{Z})$ \\ \textsf{concrete domain}};
        \node [align=center, below = of con] (ci) {\textsf{intervals and congruences}};
        \node [align=center, below right = of ci] (co) {\{$a\mathbb{Z} + b\} \cup \{\bot\}$ \\ \textsf{congruences}};
        \node [align=center, below left = of ci] (i) {$\{[a,b]\} \cup \{\bot\}$ \\ \textsf{intervals}};
        \node [align=center, below = of co] (c) {$\mathbb{Z} \cup \{\top, \bot\}$ \\ \textsf{constants}};
        \node [align=center, below = of i] (s) {$\{0, +, -, \top, \bot\}$ \\ \textsf{signs}};
        \node [align=center, below = 6em of ci] (n) {$\{0, \top, \bot\}$ \\ \textsf{nullness}};
        \node [align=center, below = of n] (d) {$\{\top, \bot\}$ \\ \textsf{dead code}};
        \node [align=center, below = of d] (t) {$\{\top\}$ \\ \textsf{no information}};

        \draw [thin] (con) -- (ci) -- (i)  -- (s) -- (n) -- (d) -- (t);
        \draw [thin] (ci) -- (co) -- (c)  -- (n);
        \draw [thin] (c) -- (i);
    \end{tikzpicture}
    \caption{Lattice of non-relational abstract domains.}
    \label{fig:latticeabs}%
\end{figure}%

So far, we have seen abstract domains with various expressiveness, cost, and
precision. However, in many cases, we want to start with the simplest domains
and refine them automatically on the fly when a spurious counterexample is
encountered. One approach of refinement is based on the observation that
also abstract domains form a lattice (see \autoref{fig:latticeabs}). In the lattice
of domains we can refine two domains by a meet operation on them -- also known
as reduced product construction \cite{Toubhans2013, Cousot2011b}. A~broader
study of large-scale reduced-products is presented in \cite{Cousot2007}.

Another approach of refinement is a \emph{powerset construction}~\cite{Cousot1979}
that builds from the elements of the abstract domain a powerset lattice. The
powerset domain allows to create a more accurate abstraction where we can
describe values by a subset of properties (e.g., in the \textit{sign} domain
the powerset domain allows to abstract a value to a subset $\{0, +\}$).  This
technique was later refined in \cite{File1999} and extended by efficient
widening operators \cite{Bagnara2004} and exacts joins of numerical elements
\cite{Bagnara2010}.

In the abstraction, the major source of imprecision is approximation induced by
abstract unions. The usual use-case of abstract disjunctions (unions) is to
merge multiple paths of the program. However, many abstract domains are not
closed under union operation.  \emph{Disjunctive completion methods} address
this problem \cite{File1999}. The complementation introduces to abstraction all
those concrete disjunctions of elements of the domain that were initially
missing.  The resulting domain is the most abstract domain that is exact on
disjunctions of abstract properties in original domain. Similarly the technique
of \emph{complementation} supplements missing complements to the abstraction
\cite{Cortesi1995}.

On the other hand, the \emph{fixpoint completion} is a program dependent refinement,
which improves the abstract domain by introducing all the properties that
would make the fixpoint iteration imprecise \cite{Giacobazzi2000,
Giacobazzi2001}.

The problem with relational abstract domains is that they do not generally
scale-up. A general technique to ensure the linear cost of abstract domains is
to split variables into independent smaller sets -- also know as \emph{variable
packing technique} \cite{Mine2006}. The variable packing can be refined by
keeping relations between the sets of variables \cite{Bouzaziz2012}.

The \emph{state partitioning} techniques \cite{Muchnick1981} work similarly,
they facilitate the use of decision-tree data structures (based on binary
decision diagrams) to split program states into smaller parts and relationally
reason about values properties only inside of those parts. The state
partitioning is commonly adopted in the state-of-the-art tools
\cite{Cousot2010, Bertrane2015}. Similarly, there are abstract domains that
incorporate the decision trees to handle disjunctions in the programs
\cite{Cousot2010a} or to prove conditional termination \cite{Urban2010}.

Further in the pursuit of increased precision, some techniques step out of the
lattice-based approach and develop a weaker formalisms for abstract domains with
the quasi-join ope\-ra\-tion~\cite{Gange2013}.

\subsection{Tools}

Most of the presented abstractions are developed as part of some abstract
interpretation framework. The prominent abstract interpreter is
Astrée~\cite{Astree}. It is a real-time embedded software static analyzer. It
can analyze C programs but without dynamic memory allocation and recursion.
Therefore, it is primarily aimed for embedded synchronous
systems~\cite{Delmas2007, Souyris2007, Bouissou2009, Bertrane2015}.  Astrée is
based on fixpoint iteration, and combines various trace semantics (i.e.,
non-relational and weakly relational abstract domains) with reduced product
refinement and widening and narrowing techniques. Astrée precision comes from
smart handling of disjunctions~\cite{Blanchet2003, Mauborgne2005} and
domain-specific abstract domains~\cite{Feret2004, Feret2005}.

Another tool SeaHorn~\cite{Gurfinkel2015} is a software verification framework
built on top of \llvm.  It is closely related to our goal, mainly because it
targets an easy design of abstract domains for program verification. The
management of abstraction in SeaHorn is provided by an abstract interpretation
module \textsc{crab} (language-agnostic library for static analysis) which is
suited for the generation of invariants into \llvm~\cite{Gershuni2019}. The
underlying abstract interpreter for \textsc{crab} library is
\textsc{ikos}~\cite{Ikos}. SeaHorn faces limitation in its usage: most
\textsc{crab} domains reason only about linear arithmetic; it lacks any pointer
or shape analysis and provides only limited support of array abstractions.
Similarly, Apron library~\cite{Jeannet2009} provides only a set of numerical
abstract domains ready for use in other tools.

\section{Towards sound analysis}
\label{sec:techniques}

Utilization of abstraction techniques is versatile; in addition to traditional
abstract interpretation, we can find many flavors of its application. In this
section, we outline the various use-cases of abstraction in the program
verification.

In verification, in comparison to static analysis, it is crucial to preserve
the soundness of the analyses. Hence, the abstraction needs to preserve all
possible paths to the error, and may create potentially new paths. We call such
abstraction over-approximating; meaning that if an abstract model is correct,
the concrete model also is.  However, there is no guarantee that the reachable
error state in the abstract model is also reachable in the original concrete
model; if not so, we are talking about the \emph{spurious} error.

\subsection{Counterexample-guided abstraction refinement}

A common technique to handle spurious coun\-ter\-examples is the
coun\-ter\-example-guided abstraction refinement (\cegar), first time presented
in~\cite{Clarke2000}. The idea of the \cegar approach is to examine a
counterexample and if it is spurious, refine the abstraction in such a way that
the spurious error would become unfeasible. Subsequently, the analysis is
restarted with refined abstraction, which is more precise and avoids the
spurious error (see \autoref{fig:cegar}).

\begin{figure}[h]
\resizebox{\textwidth}{!}{
    \begin{tikzpicture}[node distance = 4.5em, every text node part/.style={align=center}]
        \node[] (input) {\textit{program}};
        \node[component, minimum width = 8em, minimum height = 3em, right = 1em of input] (abstraction) {Abstraction};
        \node[component, minimum width = 8em, minimum height = 3em, right = of abstraction] (mc) {Model\\checking};
        \node[component, minimum width = 8em, minimum height = 3em, below = of abstraction] (refinement) {Abstraction\\Refinement};
        \node[component, minimum width = 8em, minimum height = 3em, right = of refinement] (ce) {Feasibility\\Check};
        \node[minimum height = 3em, right = of mc] (correct) {\color{apple}\textit{Correct}};
        \node[minimum height = 3em, right = of ce] (error) {\color{orioles}\textit{Error}};

        \node[fit=(abstraction) (mc) (ce) (refinement), draw, dashed] (cegar) {};

        \draw [->, >=stealth] (input) -- (abstraction);
        \draw [->, >=stealth] (abstraction) -- (mc);
        \draw [->, >=stealth] (mc) -- (correct);
        \draw [->, >=stealth] (mc) to node[right] {\textit{counter-}\\\textit{example}} (ce);
        \draw [->, >=stealth] (ce) to node[above] {\textit{feasible}} (error);
        \draw [->, >=stealth] (ce) to node[above] {\textit{spurious}} (refinement);

        \draw [->, >=stealth] (refinement) -- (abstraction);
    \end{tikzpicture}
}
\caption{Counterexample guided refinement loop.}
\label{fig:cegar}
\end{figure}

\noindent
The interesting task of \cegar is how to refine the abstraction. In the
predicate-based abstraction techniques, the refinement introduces new
predicates. Moreover, to gain more information, refinement predicates are
generalized by interpolation \cite{Jhala2006} -- see
\autoref{sec:interpolation}. The predicate abstraction together with \cegar is
an exceptionally successful abstraction technique for software model checking,
because its symbolic state representation combines well with strongest
post-conditions, and the refinement can be computed efficiently with
\textsc{smt} solvers~\cite{Beyer2013}.

As listed in the \autoref{tab:svcomp}, many state-of-the-art tools apply \cegar
in their analysis. Other applications worth mentioning are: predicate-based
model checker \textsc{blast}~\cite{Henzinger2003, Beyer2005checking}, which
implements a lazy predicate abstraction (see \autoref{sec:lazya}) on control
flow automaton of a program; \textsc{satabs} that extends the previous approach
with a \sat solver for bit-precise predicate
abstraction~\cite{Clarke2005satabs}; and combination with other techniques like
explicit-value analysis in CPAchecker~\cite{Beyer2013, Beyer2018e}.

A unified approach to the abovementioned techniques was presented in the recent
work of Beyer et al.~\cite{Beyer2018b} as a combination of model checking and
data-flow analysis. The unified framework enables an easy combination of
abstract domains, no matter whether they were invented for data-flow analysis
or for model checking. Alternatively, we can perform the refinement of abstraction
on-the-fly~\cite{Beyer2008program}.

\subsection{Lazy abstraction}
\label{sec:lazya}

A bottleneck of \cegar loop is in its repeated analysis of the program. This
problem tries to solve a lazy abstraction technique. The lazy abstraction
refines a single abstract model on-demand, so that different parts of the model
may exhibit a different level of abstraction~\cite{Henzinger2002}.
It is achieved by keeping track of both the control-flow graph, which describes how
the program locations are traversed and the data-flow, which describes what
holds at a program location. The control-flow is represented explicitly, while
the data-flow is encoded symbolically~\cite{Alberti2012}.
Similarly, as previous techniques, lazy abstraction can be improved by
interpolation~\cite{McMillan2006}. The lazy interpolant-based model checking
refines the abstraction according to refuting program paths instead of
predicate refinement as in the previous approach. By doing so, the refinement
avoids the high cost of a postcondition evaluation in the predicate approach.

Lazy abstraction comes in particularly useful to shape analysis where it allows
to refine shapes only locally~\cite{Henzinger2003,Beyer2006a}, as well as to
array abstractions~\cite{Alberti2012, Alberti2012b}.

\subsection{Symbolic abstract domains}

A different approach to abstract a set of states is to use symbolic methods:
symbolic execution~\cite{King76, Cadar2013symbolic, Baldoni2018survey}, symbolic model
checking~\cite{Clarke96, McMillan93, Cimatti20}, bounded model
checking~\cite{Biere2003bounded} or control-explicit data-symbolic model
checking (\ceds)~\cite{Mrazek2016}.

The symbolic representation can be viewed as a precise abstract domain, i.e.,
meaning that it accurately represents a set of concrete values and therefore
does not produce spurious errors. In our framework, a~symbolic domain assigns
to each variable a formula that represents constraints on it. Therefore during
the execution, we can collect all symbolic values, and by joining all
constraints, we obtain a description of an abstract state. In comparison to
\cegar approach, a symbolic execution refines its precision on-the-fly by
introducing new constraints on variables, hence remains precise throughout the
execution. The symbolic execution also features a lazy approach since whenever
it encounters an unfeasible state, it cancels exploration of current path
backtracks to the previously feasible fork on path.

In between symbolic execution tools, a prominent position holds
\textsc{klee}~\cite{Cadar2008klee}. It is a symbolic executor based on \llvm,
which enjoys widespread adoption and a lot of tools are built on top of
it~\cite{Beyer2018Klee, Chalupa2018, Chen2018, Menezes2018}. This is mainly because it
has a robust engine for system verification and support of \llvm as input
language.

Similarly, as in abstract interpretation, symbolic methods employ refinement
techniques~\cite{Beyer2016}. As many of these techniques are primarily based on
\textsc{smt} theories~\cite{Beyer2018}, the symbolic verification may apply abstraction
strategies from \textsc{smt} solvers -- for example a bitwidth
abstractions~\cite{Jonavs2018} or theory refinements~\cite{Hyvarinen2017}.

As in the case of abstraction with uninterpreted functions~\cite{Gange2016}, \textsc{smt} is also used for better approximation of abstract operations via symbolic abstraction~\cite{Thakur2012}.

\subsection{Interpolation}
\label{sec:interpolation}

Another technique used to improve abstraction-based techniques is an
interpolation. The core idea of the interpolation is to take feasible and
infeasible abstract paths, where the latter subsumes the first one, and refine
the feasible path to abstract more concrete paths but remaining still feasible
(i.e., subsumed by the infeasible state)~\cite{Mcmillan2003interpolation}.  The
interpolation is usually a part of \cegar loop as a refinement procedure for
spurious counterexamples. It has wide variety of use cases: in generation of
relevant predicates for predicate abstraction~\cite{Henzinger2004abstractions,
Cimatti2016}; range predicates in array abstraction~\cite{Jhala2007array} or
as refinement for symbolic~\cite{Ibing2016} and concolic execution~\cite{Jaffar2013}.

The interpolation was also applied in non-symbolic abstractions -- in the tool
DAGGER~\cite{Gulavani2008}, the interpolation is used to improve the precision
of widening of octagon and polyhedra domains to avoid false alarms. Similarly,
in VINTA~\cite{Albarghouthi2012}, the interpolation is used to refine the interval domain.

\section{Static analysis and transformation}

A static analysis often uses code transformation and instrumentation to
simplify its duties. A common technique is to reduce code size either directly
by a compiler~\cite{Namjoshi2018} or by program slicing~\cite{Chalupa2018}.

Another possibility is to use static analyzers to generate program invariants into the code for dynamic analysis~\cite{Gurfinkel2015} or instrument dynamic checks for memory safety analysis~\cite{Chalupa2019}.
Memory analysis also leverages instrumentation to keep numeric properties of heap-based data structures (e.g., the height of the tree, maximal element in the linked-list)~\cite{Magill2010}.

The closest approach to what we describe in the following chapters is
\emph{concolic testing} presented in CUTE~\cite{Sen2005}. CUTE instruments concolic
execution into the program, but it is restricted just to symbolic abstraction
in comparison to our generic approach.
