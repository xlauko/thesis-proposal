\chapter{State of the art}
\label{ch:state}

% Současný stav řešené problematiky, přehled klasických i aktuálních výsledků a jejich porovnání, analýza problematiky vedoucí k vymezení oblasti zájmu budoucí disertační práce, 8 - 12 stran.


% Handbook of mode checking
%   - 3.2.1.4 Level of Abstraction
%   - 3.4.1.4 Data Type Abstraction:
% The choice of how to model the data type of the flit can have a big impact on the
% scalability of verification.
%
% Chapter 13:
% Abstraction tackles this challenge based on the assumption that a reduction of
% the information content results in a reduction of the size of the representation of
% a Kripke structure.
%
% Clanky:
% The topic of constructing abstractions is
% also one of the focuses of the theory of Abstract Interpretation [8, 37–40, 76], which
% is not treated in this chapter.

% TODO prerequisites

\section{Preliminaries}
\label{sec:preliminaries}

This section introduces the notation used in the rest of the thesis proposal.
The presented notation is mainly based on the configurable software
verification technique \cite{Beyer2007, Beyer2018, Beyer2018b} and Cousot's
notation used to describe abstract interpretation \cite{Cousot2012}.

\subsection{Programs, Control-Flow Graph, States}

We restrict the presentation to a simplified version of \llvm programs
\cite{Lattner04}. An elementary unit of \llvm program is an instruction. We
distinguish binary instructions (arithmetic operations, relational operations),
memory access operations, data flow operations (branching instructions, calls,
return instructions). Because \llvmir is a typed language, it also provides
casting instructions. We admit scalar types (floats, integers), pointer types,
and aggregate types (arrays, structures).\sidenote{For detailed syntax and
semantics description see \autoref{ch:llvm}.}

The entire program in \llvm consists of global variables and functions.  A
function is a directed graph of basic blocks. A basic block is a sequence of
non-terminal instructions terminated by a terminal instruction.\sidenote{A
terminal instruction are for example branching or return instruction.}

For a more accessible representation of an \llvm program, we will use a
\emph{control flow automaton} (CFA), which is a directed graph with \llvm
instructions attached to its edges (see \autoref{fig:cfa}).

\begin{definition}
    Given a set of instructions $\mathcal{I}$, a control flow automaton
    $\mathcal{A}$ is a tuple $(\mathcal{L}, l_{\textit{init}}, \mathcal{G})$,
    where $\mathcal{L}$ is a set of program locations, $l_{\textit{init}} \in
    \mathcal{L}$ is an initial location that represents a program entry point
    and $\mathcal{G} \subseteq (\mathcal{L} \times \mathcal{I} \times
    \mathcal{L})$ is set of edges between program locations, each labeled with
    an instruction that is executed when the control flows along the edge.
\end{definition}

The set of all variables that occur in the \llvm program is denoted by
$\mathcal{V}$. A \emph{concrete state} $\sigma : \mathcal{V} \rightarrow
\mathsf{C}$ is a mapping from program variables to concrete values. We denote
the set of all concrete states as $\Sigma$. Thus, the concrete state can be
viewed as a state of program's memory that holds the value for each program
variable. A~set of admissible states at some location $l \in \mathcal{L}$ is
called a \emph{context} $\mathcal{C}_l \subseteq \mathcal{P}(\Sigma)$.

The semantics of instruction $i \in \mathcal{I}$ is defined by \emph{strongest
post condition} $\textsf{SP}_{i}(\cdot)$, i.e. it is executed on a particular program
state and accordingly updates values of program variables -- for further details
see \autoref{ch:llvm}.

A set of all edges $\mathcal{G}$ induces a transition relation $\rightarrow$ on
the set of states $\Sigma$, such that for each $g \in \mathcal{G}$ there is
$\xrightarrow{g} \: \subseteq \Sigma \times \{g\} \times \Sigma$. The
transition relation is a union over all edges $\rightarrow \bigcup_{g
\in\mathcal{G}} \xrightarrow{g}$.

A \emph{path} $\pi$ is a sequence of edges $\langle
\rightarrow_1, \rightarrow_2, \dots, \rightarrow_n \rangle$ such that all edges
form a continuous path in the transition system induced by $\mathcal{G}$.  A path
is called a \emph{program path} if it starts from initial program location
$\l_{\textit{init}}$. The semantics of path $\pi$ is defined by iterative
application of $SP_{i}( \cdot )$ for each instruction in path $\pi$. Given
initial constraints $\phi$ on variables, the strongest postcondition of path
$\pi$ is $\textsf{SP}_{\pi} = \textsf{SP}_{i_n}(\dots,\textsf{SP}_{i_1}(\phi),\dots)$. A
path $\pi$ is called \emph{feasible} if $\textsf{SP}_{\pi}(\textit{true})$ is
satisfiable and \emph{infeasible} otherwise \cite{Beyer2018b}.

A \emph{verification task} for  CFA $\mathcal{A} = (\mathcal{L},
l_{\textit{init}}, \mathcal{G})$ is to show that error location
$l_{\textit{err}} \in \mathcal{L}$ is unreachable in $\mathcal{A}$, or to find
feasible error path.

\begin{figure}

\begin{minipage}[t]{0.5\textwidth}
\begin{minted}[linenos]{llvm}
define i32 @main() {
entry:
  %x = alloca i32
  store i32 0 to i32* %x
  br label %loop
loop:
  %v = load i32* %x
  %a = add i32 %v, 1
  store i32 %a to i32* %x
  %b = icmp ult i32 %a, 5
  br %b, label %loop, label %end
end:
  ret i32 0
}
\end{minted}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\strut\vspace*{-\baselineskip}\newline\centering
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [loc, below = of l6] (l7) {$l_7$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};

    \node [loc, below = of l8] (l9) {$l_9$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);

\end{tikzpicture}
\end{minipage}

\caption{An example of \llvm program (left) and its CFA (right).}
\label{fig:cfa}
\end{figure}

\section{Abstract Interpretation}
\label{sec:abstraction}

The original idea of abstract interpretation dates back to the late 70s, first
summarized by Patrick and Radhia Cousot \cite{Cousot1977}.
They describe \emph{abstract interpretation} as a theory of abstraction and
constructive approximation of the mathematical structures used in the formal
description of programming languages or verification of undecidable program
properties~\cite{Cousot2012}.

For our course, the goal of abstract interpretation is to assign each program
location a context in a given domain, i.e., given a set of program states, the
abstraction represents properties of these states by abstract state. Program
contexts are computed by solving a system of fixpoint equations generated from
a~transition function \cite{Cousot1977}. Such computation simulates the
execution of a program in an abstract domain. As a result, each abstract context represents possible properties (invariants about states) of variables at a location after the execution of the program
\label{sec:symbolic}
with arbitrary input. The states\sidenote{State of computation comprise of all
intermediate contexts assigned to program locations.} of the computation forms
a complete lattice: $(S, \sqcap, \sqcup, \sqsubseteq, \top, \bot)$, where $S$
is a set of states, $\sqcup$, $\sqcap$ are meet and join operators,
$\sqsubseteq$ ordering of a lattice, $\top$ is the greatest element and $\bot$
it the least element of the lattice. Throughout the rest of the thesis proposal,
we will assume familiarity with the basic notions of lattice theory
\cite{Birkhoff1940}.

Given the concrete contexts from previous definitions, the \emph{concrete
semantic domain} is a complete lattice $(\mathcal{P}(\Sigma), \cap, \cup,
\subseteq, \Sigma, \emptyset)$. Even though this technique gives us a tool to
compute all possible states that a program might reach, it is, unfortunately,
uncomputable. To mitigate the problem of computability, we employ an abstract
domain instead of a concrete one. In the following sections, we will
investigate various types of abstract domains with their advantages and
disadvantages.

\section{Abstract Domains}
\label{sec:domains}

To obtain a computable model, we need to drop some information about program
variables. For instance, to drop all the information about each program
variable except whether it is positive, negative or zero. This can be achieved
by replacement of values domain we are computing on $\mathsf{C}$ by an abstract
domain $\domainm{sign} = \{ \bot, -, 0, +, \top)$ =, where $\bot$ represents an
undefined value (variable without assigned value), $+$,$-$ and $0$ denote a
\begin{marginfigure}%
    \centering
    \begin{tikzpicture}[node distance=1em]
    \node [] (t) {$\top$};
    \node [below = of t] (0) {$0$};
    \node [left = of 0] (m) {$-$};
    \node [right = of 0] (p) {$+$};
    \node [below = of 0] (b) {$\bot$};
    \draw [thin] (t) -- (m) -- (b);
    \draw [thin] (t) -- (0) -- (b);
    \draw [thin] (t) -- (p) -- (b);
    \end{tikzpicture}
    \caption{$\mathsf{A}_{\textit{sign}}$ domain lattice.}
    \label{fig:signd}%
\end{marginfigure}%
sign of variable and $\top$ is for an arbitrary value (i.e., variable can be
either negative, positive or zero).

\begin{figure}%
\begin{minipage}[t]{0.3\textwidth}
\strut\vspace*{-\baselineskip}\newline\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};
    \node [lab, right = 0cm of l1] (e1) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [lab, right = 0cm of l2] (e2) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};
    \node [lab, right = 0cm of l3] (e3) {$(0, \bot, \bot, \bot)$};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [lab, right = 0cm of l4] (e4) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [lab, right = 0cm of l5] (e5) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [lab, right = 0cm of l6] (e6) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l6] (l7) {$l_7$};
    \node [lab, right = 0cm of l7] (e7) {$(\bot, \bot, \bot, \bot)$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};
    \node [lab, right = 0cm of l8] (e8) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l8] (l9) {$l_9$};
    \node [lab, right = 0cm of l9] (e9) {$(\bot, \bot, \bot, \bot)$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);
\end{tikzpicture}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
\strut\vspace*{-\baselineskip}\newline\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};
    \node [lab, right = 0cm of l1] (e1) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [lab, right = 0cm of l2] (e2) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};
    \node [lab, right = 0cm of l3] (e3) {$(0, \bot, \bot, \bot)$};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [lab, right = 0cm of l4] (e4) {$(0, 0, \bot, \bot)$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [lab, right = 0cm of l5] (e5) {$(0, 0, +, \bot)$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [lab, right = 0cm of l6] (e6) {$(+, 0, +, \bot)$};
    \node [loc, below = of l6] (l7) {$l_7$};
    \node [lab, right = 0cm of l7] (e7) {$(+, 0, +, \top)$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};
    \node [lab, right = 0cm of l8] (e8) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l8] (l9) {$l_9$};
    \node [lab, right = 0cm of l9] (e9) {$(\bot, \bot, \bot, \bot)$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);
\end{tikzpicture}
}
\end{minipage}
\hfill
\begin{minipage}[t]{0.3\textwidth}
\strut\vspace*{-\baselineskip}\newline\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};
    \node [lab, right = 0cm of l1] (e1) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [lab, right = 0cm of l2] (e2) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};
    \node [lab, right = 0cm of l3] (e3) {$(\top, \top, \top, \top)$};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [lab, right = 0cm of l4] (e4) {$(\top, \top, \top, \top)$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [lab, right = 0cm of l5] (e5) {$(\top, \top, \top, \top)$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [lab, right = 0cm of l6] (e6) {$(\top, \top, \top, \top)$};
    \node [loc, below = of l6] (l7) {$l_7$};
    \node [lab, right = 0cm of l7] (e7) {$(\top, \top, \top, \top)$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};
    \node [lab, right = 0cm of l8] (e8) {$(\top, \top, \top, \top)$};

    \node [loc, below = of l8] (l9) {$l_9$};
    \node [lab, right = 0cm of l9] (e9) {$(\top, \top, \top, \top)$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);
\end{tikzpicture}
}
\end{minipage}
    \caption{\domain{sign} fixpoint iteration for variables
    (\llvmint{|\var{\%x}, \var{\%v}, \var{\%a}, \var{\%b}|}). A~tuple at each
    location represents an abstract evaluation of the variables in this order.
    The presented states of iteration are after first iteration
    (\textbf{left}), after 5 iterations (\textbf{middle}), reached fixpoint
    (\textbf{right}).}
    \label{fig:signcomp}%
    \add{todo page width}
\end{figure}%

We say that $\mathsf{A}$ is an abstraction of $\mathsf{C}$ because it carries
less information about program variables. One requirement on $\mathsf{A}$ is
that its values form a complete lattice (see \autoref{fig:signd}). Moreover, a
well-defined domain is required to have an image for each concrete value, and
 is over approximating abstraction. Such properties of domains are described by
 \textbf{\emph{Galois connection}}. For the concrete lattice $(C, \subseteq)$ and
 abstract lattice $(A, \sqsubseteq)$ a galois connection is $(\alpha, \gamma)$,
 where $\alpha : C \rightarrow A$, $\gamma : A \rightarrow C$ are monotonic
 functions such that:

\bigskip
\begin{center}
\begin{minipage}{0.45\textwidth}
\centering
    \emph{Abstraction \\ over-approximates}
\begin{tikzpicture}
    \node [domainbox, thick, color = apple, label=\color{apple}$C$] (c) {};
    \node [context, below = 1em of c.north] (cs) {};
    \node [below = 0.5em of cs.north] (co) {\textbullet$c$};
    \node [domainbox, thick, color = vivid, right = of c, label=\color{vivid}$A$] (a) {};
    \node [context, below = 1em of a.north] (as) {};

    \draw [->, >=stealth] (as) to [bend right=40] node[above] {$\gamma$} (cs);
    \draw [->, >=stealth] (co) to [bend right=40] node[below] {$\alpha$} (as);
\end{tikzpicture}
    \[
        \forall c \in C.\: c \subseteq \gamma( \alpha( c ) )
    \]
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
    \emph{Abstraction after concretisation yields no imprecision}
\begin{tikzpicture}
    \node [domainbox, thick, color = apple, label=\color{apple}$C$] (c) {};
    \node [context, below = 1em of c.north] (cs) {};
    \node [domainbox, thick, color = vivid, right = of c, label=\color{vivid}$A$] (a) {};
    \node [below = 0.5em of as.north] (ao) {$a$};
    \node [context, below = 1em of a.north] (as) {};

    \draw [->, >=stealth] (as) to [bend right=40] node[above] {$\gamma$} (cs);
    \draw [->, >=stealth] (cs) to [bend right=40] node[below] {$\alpha$} (ao);
\end{tikzpicture}
    \[
        \forall a \in A.\: \alpha( \gamma( a ) ) \sqsubseteq a
    \]
\end{minipage}
\end{center}

\noindent
In abstract interpretation $C$ and $A$ are called \emph{concrete} respectivelly
\emph{abstract} domain and they are assumed to be complete lattices. We call
$\alpha$ the \emph{abstraction} map and $\gamma$ the \emph{concretization} map.

Given an abstract domain and abstract operations\sidenote{In our case,
definitions how \llvm instructions transform abstract values.} we perform a
fixpoint iteration\sidenote{For detailed algorithm see \cite{Cousot1977}} and
by Knaster-Tarski theorem \cite{Tarski1955} we are guaranteed to obtain a
solution, see \autoref{fig:signcomp}.

\begin{marginfigure}%
    \centering
\resizebox{\textwidth}{!}{
    \begin{tikzpicture}[node distance=1em]
    \node [] (t) {$\top$};
    \node [below = 3em of t] (0) {$0$};
    \node [left = of 0] (m1) {$-1$};
    \node [left = of m1] (m2) {$-2$};
    \node [left = of m2] (m) {$\dots$};
    \node [right = of 0] (p1) {$1$};
    \node [right = of p1] (p2) {$2$};
    \node [right = of p2] (p) {$\dots$};
    \node [below = 3em of 0] (b) {$\bot$};
    \draw [thin] (t) -- (m) -- (b);
    \draw [thin] (t) -- (m2) -- (b);
    \draw [thin] (t) -- (m1) -- (b);
    \draw [thin] (t) -- (0) -- (b);
    \draw [thin] (t) -- (p1) -- (b);
    \draw [thin] (t) -- (p2) -- (b);
    \draw [thin] (t) -- (p) -- (b);
    \end{tikzpicture}
}
    \caption{\domain{cp} domain lattice.}
    \label{fig:cp}%
\end{marginfigure}%

Even though our example program in \autoref{fig:signcomp} is quite trivial, the
sign domain \domain{sign} was not able to preserve any interesting properties,
and all resulting contexts contains either $\top$ or $\bot$.  Sign domain belongs to
the class of simple domains that reason only about single property of variables --
also known as \emph{property domains}. Another famous example of a property
domain is a \textbf{\emph{parity domain}}, which captures whether the
variable is odd or even. The parity domain is just a special case of a
\textbf{\emph{congruence domain}} \cite{Granger1989, Granger1991}.

The basic building block of these domains is \emph{basis} that expresses how to
abstract one program variable and simple arithmetic operations. Thanks to
the galois connection the rest of interpretation can be derived automatically
\cite{Mine2004Thesis}.

A~\textbf{\emph{constant propagation domain}} \domain{cp} (see
\autoref{fig:cp}) is another example of property domain used in optimization
techniques \cite{Kildall1973}.  It is possible to combine multiple domains to
reason about multiple properties either by keeping each property separately or
by reduced product of domains (see \autoref{fig:cpsign}), which is more precise
and more efficient than multiple separate interpretations \cite{Cousot2011b}.
For further information see \autoref{sec:domainrefinement}.

\begin{marginfigure}%
    \centering
\resizebox{\textwidth}{!}{
    \begin{tikzpicture}[node distance=1em]
    \node [] (t) {$\top$};
    \node [below = 6em of t] (0) {$0$};
    \node [left = of 0] (m1) {$-1$};
    \node [left = of m1] (m2) {$-2$};
    \node [above = 3em of m2] (mm) {$-$};
    \node [left = of m2] (m) {$\dots$};
    \node [right = of 0] (p1) {$1$};
    \node [right = of p1] (p2) {$2$};
    \node [right = of p2] (p) {$\dots$};
    \node [above = 3em of p2] (pp) {$+$};
    \node [below = 3em of 0] (b) {$\bot$};
        \draw [thin] (t) -- (mm) -- (m)  -- (b);
        \draw [thin] (t) -- (mm) -- (m2) -- (b);
        \draw [thin] (t) -- (mm) -- (m1) -- (b);
    \draw [thin] (t) -- (0) -- (b);
        \draw [thin] (t) -- (pp)-- (p1) -- (b);
        \draw [thin] (t) -- (pp)-- (p2) -- (b);
        \draw [thin] (t) -- (pp) -- (p) -- (b);
    \end{tikzpicture}
}
    \caption{\domain{cs} is a joint sign (\domain{sign}) and constant propagation (\domain{cp}) domain.}
    \label{fig:cpsign}%
\end{marginfigure}%

Probably the simplest of domains is a \emph{definedness domain} \domain{D}, which only
reason about definedness of values (see domain lattice \autoref{fig:unitd}).
Despite its simplicity, it can be useful to detect reachability of program
locations. Even simpler domain is a~single value domain \domain{U}. The single
value domain is usefull if we want to omit a variable from the model of the
program.

\begin{marginfigure}%
    \centering
    \begin{tikzpicture}[node distance=1em]
    \node [] (t) {$\top$};
    \node [below = of t] (b) {$\bot$};
    \draw [thin] (t) -- (b);
    \end{tikzpicture}
    \caption{\domain{U} tracks only definedness of variables.}
    \label{fig:unitd}%
\end{marginfigure}%

Since the precision of an abstract domain highly depends on the program
structure, many abstract domains were designed to track various program
properties. Let us take it chronologically. The first domain presented in
Cousot's paper \cite{Cousot1977} to achieve more precise results was an
\textbf{\emph{interval domain}} \domain{I}. In the interval abstraction, a set of values
is represented by its minimal and maximal value. Since in some cases, it is not
possible to determine the boundary values, the interval domain utilizes
infinities to denote arbitrary bound -- we represent arbitrary value
$\top_{\textit{I}}$ by $[-\infty, \infty]$ and undetermined value $\bot_{\textit{I}}$
by empty interval $\emptyset$.

\begin{marginfigure}
\strut\vspace*{-\baselineskip}\newline\centering
\resizebox{\textwidth}{!}{
\begin{tikzpicture}[node distance=1.1em]
    \node [loc] (l1) {$l_1$};
    \node [lab, left = 0cm of l1] (entry) {\llvmint{entry:}};
    \node [lab, right = 0cm of l1] (e1) {$(\bot, \bot, \bot, \bot)$};

    \node [loc, below = of l1] (l2) {$l_2$};
    \node [lab, right = 0cm of l2] (e2) {$(\bot, \bot, \bot, \bot)$};
    \node [loc, below = of l2] (l3) {$l_3$};
    \node [lab, left = 0cm of l3] (loop) {\llvmint{loop:}};
    \node [lab, right = 0cm of l3] (e3) {$([0,0], [0,0], [0,0], [0,0])$};

    %\node[fit=(l1) (l2), draw, dashed] (ebb) {};

    \node [loc, below = of l3] (l4) {$l_4$};
    \node [lab, right = 0cm of l4] (e4) {$([0,0], [0,0], [1,1], [0,1])$};
    \node [loc, below = of l4] (l5) {$l_5$};
    \node [lab, right = 0cm of l5] (e5) {$([0,0], [0,0], [1,1], [0,1])$};
    \node [loc, below = of l5] (l6) {$l_6$};
    \node [lab, right = 0cm of l6] (e6) {$([1,1], [0,0], [1,1], [0,1])$};
    \node [loc, below = of l6] (l7) {$l_7$};
    \node [lab, right = 0cm of l7] (e7) {$([1,1], [0,0], [1,1], [0,1])$};

    %\node[fit=(l3) (l4) (l5) (l6) (l7), draw, dashed] (lbb) {};

    \node [loc, below = of l7] (l8) {$l_8$};
    \node [lab, left = 0cm of l8] (loop) {\llvmint{end:}};
    \node [lab, right = 0cm of l8] (e8) {$([0,0], [0,0], [0,0], [0,0])$};

    \node [loc, below = of l8] (l9) {$l_9$};
    \node [lab, right = 0cm of l9] (e9) {$([0,0], [0,0], [0,0], [0,0])$};

    %\node[fit=(l8) (l9), draw, dashed] (lbb) {};

    \draw [->, >=stealth] (l1) -- node[midway, right] {\llvmint{|\var{\%x}| = alloca i32}} (l2);
    \draw [->, >=stealth] (l2) -- node[midway, right] {\llvmint{store 0 to |\var{\%x}|}} (l3);
    \draw [->, >=stealth] (l3) -- node[midway, right] {\llvmint{|\var{\%v}| = load |\var{\%x}|}} (l4);
    \draw [->, >=stealth] (l4) -- node[midway, right] {\llvmint{|\var{\%a}| = add |\var{\%v}| 1}} (l5);
    \draw [->, >=stealth] (l5) -- node[midway, right] {\llvmint{store |\var{\%a}| to |\var{\%x}|}} (l6);
    \draw [->, >=stealth] (l6) -- node[midway, right] {\llvmint{|\var{\%b}| = ult |\var{\%a}| 5}} (l7);
    \draw [->, >=stealth] (l7) -- node[midway, right] {\llvmint{[|\var{\%b}| = false]}} (l8);
    \draw [->, >=stealth] (l7) to [bend left=40] node[above, rotate = 90] {\llvmint{[|\var{\%b}|= true]}} (l3);
    \draw [->, >=stealth] (l8) -- node[midway, right] {\llvmint{ret 0}} (l9);
\end{tikzpicture}
}
    \caption{TODO correct intervals: Interval abstraction example after fixpoint iteration.}
\end{marginfigure}
\add{ example of interval abstraction on example program }

Unfortunately, with the infinite lattice of the interval
domain,\sidenote{The interval domain does not meet ascending chain condition.} the
abstract interpretation will obtain a possible infinite or impractically long
executions (e.g., when the interpretation can not determine the bound of a loop
it can increment the boundary of an interval indefinitely never reaching the
fixpoint). For this reason, abstract interpreters employ widening and
narrowing techniques to accelerate convergence \cite{Cousot1992a, Cortesi2011}.
The idea of wi\-de\-ning is to overshoot the least fixpoint after few
unsuccessful iterations of the interpretation and subsequently by narrowing to
refine the over-approximated solution.
\begin{definition}
    \textbf{Widening operator} $\nabla$ for domain $(\mathcal{A},
    \sqsubseteq)$ fulfills:
    \begin{enumerate}
        \item $\forall v_1, v_2 \in \mathcal{A} : v_1 \sqcup v_2 \sqsubseteq v_1 \nabla v_2$,
        \item for each sequence $(v_k)_{k \in \mathbb{N}}$, the sequence $(v^{\nabla}_k)_{k \in \mathbb{N}}$ defined as $v^{\nabla}_0 = v_0$ and $v^{\nabla}_k = v^{\nabla}_{k-1} \nabla v_k$ reaches a fixpoint after finitely many steps.
    \end{enumerate}
\end{definition}
\begin{definition}
    \textbf{Narrowing operator} $\Delta$ for domain $(\mathcal{A},
    \sqsubseteq)$ fulfills:
    \begin{enumerate}
        \item $\forall v_1, v_2 \in \mathcal{A} : v_1 \sqsupseteq v_2 \implies v_1 \Delta v_2 \sqsubseteq v_2$,
        \item for each sequence $(v_k)_{k \in \mathbb{N}}$, the sequence $(v^{\Delta}_k)_{k \in \mathbb{N}}$ defined as $v^{\Delta}_0 = v_0$ and $v^{\Delta}_k = v^{\Delta}_{k-1} \Delta v_k$ reaches a fixpoint after finitely many steps.
    \end{enumerate}
\end{definition}

By different implementation of widening and narrowing, abstract interpreters
employ different strategies to achieve convergence -- for example widening
with thresholds \cite{Blanchet2003, Lakhdar2011}, delayed widening, parma widening
\cite{Bagnara2008} or abstract acceleration \cite{Gonnord2006, Feautrier2010}.

An alternative approach to tackle infinite interpretation that does not require
any extrapolation operator is a \textbf{\emph{policy iteration}}
\cite{Costan2005, Gaubert2007, Gawlitza2007, Gawlitza2007b, Gawlitza2011}. The
idea of policies is to compute fixpoint solution of a sequence of simpler
semantic equations, such that the least fixpoint is reached after a finite
number of iterations.  The sequence of policies defines a strategy to approach
the fixpoint either from above or below. Policies are formed from a
decomposition of original abstraction.

\begin{marginfigure}
\begin{minted}[linenos]{c}
int i = 10;
int v = 0;
while (i >= 0) {
    i = i - 1;
    if (random())
        v = v + 1;
}
\end{minted}
    \caption{Program that requires a relational invariant.}
    \label{fig:relationalc}
\end{marginfigure}

The drawback of the domains presented uptil now is that they are not able to reason about
relational properties between variables -- hence we address them as
\emph{non-relational} abstract domains.
However, many verification tasks require to take relations into account.
For example, to show that $v \leq 11$ at line 7 in program of
\autoref{fig:relationalc}, we need to prove a relational loop invariant $v + i
\leq 10$.

%\add{other applications of relational domains: analysis of programs with
%symbolic parameters, modular analysis of procedures, inference of non-uniform
%non-numerical invariants (pointer analysis)}

\subsection{Relational abstract domains}

The simplest of the relational abstract domains is a \emph{linear equality
domain} that captures information about affine relationships among program
variables \cite{Karr1976}. In this domain, an abstract variable is represented
by the affine subspace of program state space. The problem with the affine
domain is in its nonunique minimal form; this problem is solved by combination
with the congruence domain \cite{Granger1991}.

Another relational domain is a \emph{polyhedron abstract domain} which is
inspired by the theory of linear programming and optimization \cite{Cousot1978, Venet2012}
and provides a unique representation. Though, the manipulation with polyhedron
abstract values involves costly computation of a simplex algorithm
\cite{Schrijver1986}. Nowadays a polyhedron abstract domain is still
extensively studied -- an abstraction of polyhedron by bounding boxes to
simplify abstract operations \cite{Singh2017}, or in combination with interval
domain to reason about interval linear properties \cite{Chen2009}.

\subsection{Weakly relational abstract domains}

In the middle between relational and non-relational abstract domains stand
\textbf{\emph{weakly relational abstract domains}}. These domains aim to
increase the precision of non-relational domains (interval domain) and reduce
the cost of abstraction of relational domains (polyhedron domain) by keeping
only a reduced amount of relations.

The notable representant of weakly-relational domains is an \emph{octagon
abstract domain} \cite{Mine2006}. This domain efficiently enables the representation
of invariants of form $\pm x \pm y \leq c$, where $x$ and $y$ are program
variables and $c$ is a constant.  Moreover, the advantage of the octagon domain
is in its representation by potential graphs and difference-bound matrices
\cite{Larsen1997}, which allows a unique minimal representation of abstract
values. The octagon domain was further optimized by bucketing of octagons
\cite{Blanchet2003, Venet2004} and modified to extend captured properties
\cite{Claris2004, Mine2004}. Further restriction provides a zone domain
\cite{Mine2001} which reasons only about invariants of form $x - y \leq c$ and
$\pm x \leq c$.

A less precise and lightweight relational domain of pentagons was introduced in
\cite{Logozzo2010} with applicability on validation of array manipulating
byte-code. The pentagon domain captures properties of the form of $x \in [a,b]
\wedge x < y$.

A recent abstract domain utilizes parallelotopes to encode any linear
constraint as the polyhedra, while it preserves the efficiency of weakly
relational abstract domains \cite{Amato2017}.

A disadvantageous aspect of relational abstract domains is their complicated
representation and therefore computationally less efficient abstract
operations. In addition to maintaining more properties about variables, the
representation also needs to assure uniqueness of the representation. Hence the
operations need to incorporate some sort of normalization (e.g., difference
bound matrices in the octagon domain \cite{Mine2006}, or generators of affine
space in the linear equality domain \cite{Karr1976}).

Except picking of a suitable abstraction, a precision of abstract methods can
be improved by symbolic methods. Miné presents in his work \cite{Mine2006b} a
set of lightweight simplifications (linearization and symbolic constant
propagation) which are used to transform numerical expressions in order to
increase the precision of the analysis. Likewise, Chang takes use of congruence
closure extensions~\cite{Chang2005}.

\subsubsection{Predicate Abstraction}

In program verification, predicate abstraction \cite{Graf1997, Bjorner1997} is
a prominent technique used to handle infinite state-spaces (see
\autoref{tab:svcomp}). Elements of the abstract domain are abstract predicates
over program variables. The interpretation of predicates is a set of program
states (i.e., the states of abstracted system correspond to truth assignments
to a set of predicates). Because the predicate abstraction is restricted to a
finitary domain, its transition function can be computed by \smt solver/theorem
prover~\cite{Cousot2003, Ball2001}. The predicates used in abstraction are
generated in a heuristic manner from the program under test or by a refinement
loop (see TODO) \cite{Flanagan2002}. In contrast to previous abstract domains,
a predicate representation may not just reason about a single variable but a
whole program state (set of states), moreover predicates inherently keep
relational information between variables.

\add{ extend citations }

\subsubsection{Domain of uninterpreted functions}

The domain that is of particular interest for us in the following chapters is a
domain of \emph{uninterpreted functions}~\cite{Gange2016}. This domain is targeted
at problems of relational abstract domains -- poor scalability and loss of
tracking of non-linear properties. The idea of the domain is to treat program
instructions as uninterpreted function symbols -- i.e., built expression trees,
where leaves are nullary terms (program variables) and nodes represent n-ary
operations. This domain can be used to augment non-relational abstract domains
with relational information.

\bigskip
\prule
\bigskip

\noindent
So far, we have presented only domains, which abstract only scalar values -- we call
them numerical abstract domains.  Overall a \emph{numerical abstract domain} is
determined by three primary components:

\phantomsection
\label{sec:ingredients}
\begin{enumerate}
    \item a poset $(\mathcal{A}, \sqsupseteq)$ with concretization and abstraction function,
    \item effective and sound abstract operators,
    \item an iteration strategy.
\end{enumerate}

\subsection{Non-numerical abstract domains}

Generally known as symbolic abstractions, they abstract infinite sets of
in/finite functions \cite{Cousot14}, graphs and other constructs of programming languages like
arrays~\cite{Cousot2011p}, trees~\cite{Mauborgne2000}, heaps (non sharing
\cite{Cousot1977static, Cousot1977rec} or shape analysis~\cite{Reps2002, Chang2008}).

\subsubsection{Memory abstraction}

To perform memory abstractions, we extend our model to consider a program heap.
In memory aware analysis, we have a set of values $\mathcal{C}$; memory addresses are
values $C_{\textit{addr}} \subseteq C$. The environment $e \in \mathcal{E}$
maps program variables into their addresses: $\mathcal{E} = \mathcal{V} \rightarrow \mathcal{C}_{\textit{addr}}$.
Then heap $h \in \mathcal{H}$ is a partial function that map addresses into
values: $\mathcal{H} = \mathcal{C}_{\textit{addr}} \rightarrow \mathcal{C}$.
A particular environment and heap describe a memory state $\sigma_m \in
\mathcal{M}$ where $\mathcal{M} = \mathcal{E} \times \mathcal{H}$.

\change{ zjednotit s uvodom }

\add{ maybe create memory example }

\subsubsection{Pointer abstraction}

In the memory-aware analysis, we may perform a separate abstraction of
pointers. A simplest of them are non-relational pointer abstractions (see null
pointer abstraction \autoref{fig:nulldomain}). A pointer abstraction is defined
in the same way as a scalar abstraction; it is given as a set of abstract
pointer values $A_{\textit{ptr}}$; concretization function
$\gamma_{\textit{ptr}} : A_{\textit{ptr}} \rightarrow
\mathcal{P}(\mathcal{C}_{\textit{addr}})$ and abstraction function
$\alpha_{\textit{ptr}} : \mathcal{P}(\mathcal{C}_{\textit{addr}}) \rightarrow
A _{\textit{ptr}}$.

\begin{marginfigure}

    \add{ add lattice }
    \caption{Null pointer domain keeps information about null-pointers.}
    \label{fig:nulldomain}
\end{marginfigure}

Non-Relational pointer domains have disadvantage because of their imprecision -- as soon as a points-to set of pointers contain several elements, the abstract interpretation performs imprecise updates. Moreover, they are not able to express well-structured invariants about memory objects (e.g., lists, tree).

For more precise updates, pointer abstraction leverage separation logic principle. It allows performing local reasoning about memory (i.e., consider only a part of memory). In separation logic, the memory relations are described by points-to predicates and separating conjunctions on heaps~\cite{Reynolds2002}.

Just separation logic is too expressive to be tractable as an abstract domain because it is not efficient for manipulation. In static analysis, separation logic is usually used in combination with shape analysis. Shape graphs are used to generalize abstract data structures (e.g., lists or trees). The shape graph can summarize inductive data structures into simple shapes~\cite{Distefano2006}. Similarly to scalar analysis, shape analysis can be refined by widening or by keeping shape relations~\cite{Chang2008}.
Reduced products to shape analysis was introduced in~\cite{Toubhans2013}.

More recent approaches leverage \smt solvers and reduce problems from
separational logic to propositional logics~\cite{Piskac2013, Itzhaky2014,
Itzhaky2014b}.  Also, template-based domains~\cite{Malik2018} delegates
semantic reasoning to \smt solvers and concentrates on the design of
appropriate shape templates.  In comparison to traditional predicate analysis
where a user needs to supply a set of predicates, template-based domains use a
set of parametrized predicates which are further gradually refined via \smt.
Moreover, the \smt representation enables a straightforward combination with
value analysis -- hence allows reasoning about the shape and their contents at
the same time.

To reason only about numeric properties of heap manipulating programs, we can omit a lot of shape analysis. Magill et al. present an abstraction~\cite{Magill2010} that instruments only numeric abstraction of shape properties into the program, hence allow cheaper reasoning about properties like a length of linked-lists or distance between two elements of the list. Other domains extend the relational numeric analysis of the content of particular shapes, for example, trees~\cite{Journault2019}.

Many tools specialize in the analysis of a single type of data-structure. To name a few: Predator is a tool for automated verification of programs with dynamic linked data structures using a separation logic and shape graphs~\cite{Dudka2012}; Forester leverage tree automata in combination with shape analysis to verify programs with various types of lists or trees~\cite{Holik2015}; Space Invader~\cite{Yang2008}.

\add{ extend }

For an extended survey of heap abstractions, we refer to~\cite{Kanvar2016}

\subsubsection{Array abstraction}

The simplest and heavily used data structure -- array gets a lot of specific
abstract domains for efficient reasoning about array manipulating programs. The
analyzer needs to be able to discover relationships among values of array
elements, as well as their relationships to scalar variables and constants.

It is problematic to statically reason about arrays, because of their unbounded
nature.  Usually, the array operations do not consider a particular fixed array
size. Instead, the array size depends typically on the scalar variables. The
precise verification of such program requires being able to reason about
relationships between array size and array content and program variables. Hence
analyses tend to utilize symbolic methods to represent constraints on
arrays~\cite{Gopan2005}.

Generally, we recognize two types of array-related techniques. Depending on the
properties, analysis focus either on the reasoning about array bounds and
related out of bounds errors, or on the content of the array~\cite{Halbwachs2008}.

To reason about array content, abstract domains are usually parametrizable by
numerical abstract domains~\cite{Cousot2011p}. Symbolic methods are also used
to represent or generalize facts about array manipulations -- for example,
\emph{universally quantified abstract domains}, enable to quantify over array
elements with the use of simpler abstract domains~\cite{Gulwani2008}.
Predicate abstraction is also used to describe properties about arrays in a
form of update predicates~\cite{Kovacs2009}.

The symbolic techniques naturally request for refinement of abstractions.
Refinement of universally quantified abstract domains is presented in~\cite{Seghir2009}.
As universal quantifiers represent hard task for decision procedures; the
refinement tries to eliminate their occurrences using ghost variables.
Refinement techniques leverage that predicates have a particular form when they
reason about array indices. Other refinement approaches utilize lazy-abstraction~\cite{Alberti2012} and interpolants~\cite{Alberti2012b}.

The recent work~\cite{Payet2018} summarizes how weakly-relational domains for
array indices does not scale in the analysis of object-oriented languages.
However, in combination with symbolic expressions, traditional domains can
scale to real-world codebases.

A different approach is via program instrumentation~\cite{Cornish2015} to lower
program arrays. The array-free programs can then leverage the fine-tuned
numerical analysis. Moreover, the instrumentation can slice program only for
interested array properties.

\add{ reword + extend paper description }

\subsubsection{String abstractions}

Even though array abstraction might be used to handle strings in programs, it
is more precise to treat them separately. In C/C++ language strings are
represented as null-terminated arrays of characters. Hence, to detect string
related errors, many abstractions extends the array abstractions to keep record
about null characters~\cite{Olliaro2018}.

Since string domains are more restricted, they utilize other formalisms to
represent an abstract set of strings like finite
automata~\cite{Christensen2003} or trie data structures.

The recent work of Constantini~\cite{Costantini2011, Costantini2015} proposes a
unifying approach to string analysis.  Presented domains can be tuned to a
different level of precision and address only specific properties of strings.

\add{ extend constantini work description + example }

\subsection{Real-world programs dedicated abstractions}

In addition to general-purpose domains presented so far, verification of
real-world programs delivered many domain-specific abstractions. Many numerical
domains work with unbounded numbers. However, we would like to reflect the
properties of bit-precise arithmetic like overflows.

Recent domains are based on bit-vector arithmetic and reproduce processor semantics~\cite{Sharma2013, Sharma2017}. Similar domains were also designed for floating-point arithmetic~\cite{Putot2004}.

\add{ extend + example of difference ? }

The paper~\cite{Reps2010} outlines challenges that arise when analyzing machine code and what can be done about them.
\add{ describe challenges }

More domain-specific abstractions are used to analyze database querying languages~\cite{Halder2012} or system components like filters~\cite{Feret2004} or synchronous clocks~\cite{Feret2005}.

A real-world program analysis also requires to reflect memory model of the program under test. To describe memory model properties, analysis usually leverage dedicated predicate abstraction~\cite{Dan2013}.
%\add{ \cite{Majumdar2018} }
\add{extend memory model abstraction}

\subsection{Domain Refinement}
\label{sec:domainrefinement}

\begin{marginfigure}%
    \centering
\resizebox{\textwidth}{!}{
    \begin{tikzpicture}[node distance=2em]
        \node [align=center] (con) {$\mathcal{P}(\mathbb{Z})$ \\ \textsf{concrete domain}};
        \node [align=center, below = of con] (ci) {\textsf{intervals and congruences}};
        \node [align=center, below right = of ci] (co) {\{$a\mathbb{Z} + b\} \cup \{\bot\}$ \\ \textsf{congruences}};
        \node [align=center, below left = of ci] (i) {$\{[a,b]\} \cup \{\bot\}$ \\ \textsf{intervals}};
        \node [align=center, below = of co] (c) {$\mathbb{Z} \cup \{\top, \bot\}$ \\ \textsf{constants}};
        \node [align=center, below = of i] (s) {$\{0, +, -, \top, \bot\}$ \\ \textsf{signs}};
        \node [align=center, below = 11em of ci] (n) {$\{0, \top, \bot\}$ \\ \textsf{nullness}};
        \node [align=center, below = of n] (d) {$\{\top, \bot\}$ \\ \textsf{dead code}};
        \node [align=center, below = of d] (t) {$\{\top\}$ \\ \textsf{no information}};

        \draw [thin] (con) -- (ci) -- (i)  -- (s) -- (n) -- (d) -- (t);
        \draw [thin] (ci) -- (co) -- (c)  -- (n);
        \draw [thin] (c) -- (i);
    \end{tikzpicture}
}
    \caption{Lattice of non-relational abstract domains.}
    \label{fig:latticeabs}%
\end{marginfigure}%


So far, we have seen abstract domains with various expressiveness, cost, and
precision. However, in many cases, we want to start with the simplest domains
and refine them automatically on the fly when a spurious counter-example is
encountered. The first approach of refinement is based on the observation that
also abstraction form a lattice (see \autoref{fig:latticeabs}). In the lattice
of domains we can refine two domains by a meet operation on them -- also known
as reduced product construction \cite{Toubhans2013, Cousot2011b}. A broader
study of large-scale reduced-products is presented in \cite{Cousot2007}.

The powerset construction of domains \cite{Cousot1979} is also refined in later works
\cite{File1999} and extended by efficient widening operators \cite{Bagnara2004}
and exacts joins of numerical elements \cite{Bagnara2010}.

Many abstract domains are not closed under union. Usually, the approximation
induced by abstract unions is a major source of precision loss. Disjunctive
completion methods address this problem \cite{File1999}. The complementation
introduces to abstraction \domain{} all those concrete disjunctions of elements
of \domain{} that were initially missing. The resulting domain is the most
abstract domain that is exact on disjunctions of abstract properties in
\domain{}.  The usual usecase of abstract dijunctions is to merge multiple paths
of the program.  Similarly complementation supplements missing complements to the
abstraction \cite{Cortesi1995}.

On the other hand, the fixpoint completion is a program dependent refinement,
which improves the abstract domain by introducing all the properties which
would make the fixpoint iteration imprecise \cite{Giacobazzi2000,
Giacobazzi2001}.

The problem with relational abstract domains is that they do not generally
scale-up. A general technique to ensure the linear cost of abstract domains is
to split variables into independent smaller sets -- also know as variable
packing technique \cite{Mine2006}. The variable packing can be refined by
keeping relations between the sets of variables \cite{Bouzaziz2012}.

The state partitioning techniques \cite{Muchnick1981} facilitate the use of
decision-tree data structures (based on binary decision diagrams) which are
used in the state of the art tools \cite{Cousot2010, Bertrane2015}.
Similarly, there are abstract domains that incorporate the decision trees to
handle disjunctions in the programs \cite{Cousot2010a} or to prove conditional
termination \cite{Urban2010}.

Further in the pursuit of increased precision, some techniques step out of the
lattice-based approach and develop a weaker formalism for abstract domains with
the quasi-join ope\-ra\-tion~\cite{Gange2013}.

\subsection{ Tools }

Most of the presented abstractions are developed as part of some abstract
interpretation framework. The prominent abstract interpreter is
Astrée~\cite{Astree}. It is a real-time embedded software static analyzer. It
can analyze C programs but without dynamic memory allocation and recursion.
Therefore, it is primarily aimed for embedded synchronous systems of electric
flight control~\cite{Delmas2007, Souyris2007, Bouissou2009, Bertrane2015}.
Astrée is based on fixpoint iteration, and combines various trace semantics
(i.e., non-relational and weakly relational abstract domains) with reduced
product refinement and widening and narrowing techniques. Astrée precision
comes from smart handling of disjunctions~\cite{Blanchet2003,
Mauborgne2005} and domain-specific abstract domains~\cite{Feret2004,
Feret2005}.

Another tool SeaHorn~\cite{Gurfinkel2015} is a software verification framework
built on top of \llvm.  It is closely related to our curse, mainly because it
provides an abstract interpretation module \textsc{crab} (language-agnostic
library for static analysis) which is suited for the generation of invariants
into \llvm programs~\cite{Gershuni2019}. The underlying abstract interpreter
for \textsc{crab} library is \textsc{ikos}~\cite{Ikos}. SeaHorn faces
limitation in its usage: most \textsc{crab} domains reason only about linear
arithemtic; it lacks any pointer or shape analysis and provides only limited
support of array abstractions.

\section{Abstraction based techniques}
\label{sec:techniques}

Utilization of abstraction techniques is versatile; in addition to traditional
abstract interpretation, we can find many flavors of its application. In this
section, we outline the various use-cases of abstraction in the program
verification.

\subsection{Abstraction-based model checking}

In model checking, abstraction is used to reduce the size of state space~\cite{Pelnek2006}.
Instead of performing exploration of concrete state space, algorithms group a
set of concrete states together according to some abstraction to explore a
reduces state space (see \autoref{fig:mcabstraction}). Model checkers also use
abstraction or symbolic representation to handle non-deterministic data instead
of exhaustive enumeration of all possible values~\cite{Mrazek2016}.

\begin{marginfigure}

\add{ picture }

\caption{State space abstraction.}
\label{fig:mcabstraction}
\end{marginfigure}

In general, we are talking about overapproximating abstraction; which means
that if an abstract model is correct, the concrete model also is. However,
there is no guarantee that reachable error state in the abstract model is also
reachable in the concrete one; if not so, we are talking about the spurious error.

\subsection{Counterexample-guided abstraction refinement}

A common technique to handle spurious counterexamples is counterexample guided
abstraction refinement (\cegar), first time presented in~\cite{Clarke2000}.
The idea of the \cegar approach is to examine a counterexample and if it is spurious,
refine the abstraction. Subsequently, we restart the analysis with abstraction,
which is more precise and avoids the spurious error (see \autoref{fig:cegar}).
% A Practical and Complete Approach to Predicate Refinement

\begin{figure}[h]
\resizebox{\textwidth}{!}{
    \begin{tikzpicture}[node distance = 4.5em, every text node part/.style={align=center}]
        \node[] (input) {\textit{program}};
        \node[component, minimum width = 8em, minimum height = 3em, right = 1em of input] (abstraction) {Abstraction};
        \node[component, minimum width = 8em, minimum height = 3em, right = of abstraction] (mc) {Model\\checking};
        \node[component, minimum width = 8em, minimum height = 3em, below = of abstraction] (refinement) {Abstraction\\Refinement};
        \node[component, minimum width = 8em, minimum height = 3em, right = of refinement] (ce) {Feasibility\\Check};
        \node[minimum height = 3em, right = of mc] (correct) {\color{apple}\textit{Correct}};
        \node[minimum height = 3em, right = of ce] (error) {\color{orioles}\textit{Error}};

        \node[fit=(abstraction) (mc) (ce) (refinement), draw, dashed] (cegar) {};

        \draw [->, >=stealth] (input) -- (abstraction);
        \draw [->, >=stealth] (abstraction) -- (mc);
        \draw [->, >=stealth] (mc) -- (correct);
        \draw [->, >=stealth] (mc) to node[right] {\textit{counter-}\\\textit{example}} (ce);
        \draw [->, >=stealth] (ce) to node[above] {\textit{feasible}} (error);
        \draw [->, >=stealth] (ce) to node[above] {\textit{spurious}} (refinement);

        \draw [->, >=stealth] (refinement) -- (abstraction);
    \end{tikzpicture}
}
\add{ fit page width }
\caption{\cegar loop}
\label{fig:cegar}
\end{figure}

The interesting task of \cegar is how to refine abstraction. In predicate
abstraction-based techniques, a refinement introduces new predicates, however to
gain a more information from refinement, predicates are interpolated
\cite{Jhala2006} -- see \autoref{sec:interpolation}. The predicate abstraction
is an exceptionally successful abstraction technique for software model
checking, because its symbolic state representation combines well with
strongest post-conditions, and abstractions can be computed efficiently with
\smt solvers~\cite{Beyer2013}.

As listed in the \autoref{tab:svcomp}, many state-of-art tools apply \cegar in
their analysis. Other applications worth mentioning are: predicate-based model
checker \textsc{blast}~\cite{Henzinger2003, Beyer2005checking}, which
implements a lazy predicate abstraction (see \autoref{sec:lazya}) on control
flow automaton of a program; \textsc{satabs} that extends the previous approach
with a \sat solver for bit-precise predicate abstraction~\cite{Clarke2005satabs};
and combination with other techniques like explicit-value analysis in
CPAchecker~\cite{Beyer2013, Beyer2018e}.

A unified approach to the abovementioned techniques was presented in the recent
work of Beyer et al.~\cite{Beyer2018b} as a combination of model checking and
data-flow analysis. The unified framework enables an easy combination of
abstract domains, no matter whether they were invented for data-flow analysis
or for model checking.

Alternatively, we can perform the refinement of abstraction
on-the-fly~\cite{Beyer2008program}.

\subsection{Lazy abstraction}
\label{sec:lazya}

A bottleneck of \cegar loop is in its repeated analysis of the program. This
problem tries to solve a lazy abstraction technique. The lazy abstraction
refines a single abstract model on-demand, so that different parts of the model
may exhibit a different level of abstraction~\cite{Henzinger2002}.\sidenote{Lazy
abstraction maintains different degrees of precision for different parts of the
program by keeping track of both the control-flow graph, which describes how
the program locations are traversed and the data-flow, which describes what
holds at a program location.  The control-flow is represented explicitly, while
the data-flow is encoded symbolically~\cite{Alberti2012}.} The application of
interpolants further improved it in~\cite{McMillan2006}. The lazy
interpolant-based model checking refines the abstraction according to refuting program paths
instead of predicate refinement as in the previous approach. By doing so,
the refinement avoids the high cost of a postcondition evaluation in the
predicate approach.

This technique had particular success in shape analysis where it enables
the refinenement of shapes only locally~\cite{Henzinger2003,Beyer2006a} as well as in array
abstractions~\cite{Alberti2012, Alberti2012b}.

\subsection{Eager Abstraction}

On the contrary to lazy abstraction, which has used a refinement of abstraction
on demand, the eager abstraction~\cite{McMillan2018} is inspired by eager
theory explication used in \smt solvers. Eager strategy in \smt solvers
abstract the formula to its propositional skeleton -- to compensate the loss of
information \smt solver adds tautologies to the formula before abstraction,
whereas the lazy approach produces tautologies retroactively after abstraction.
In the case of model checking, we apply the eager abstraction on the level of
atomic propositions. The eager abstraction separates propositional and theory
reasoning about program states -- by the skeletal abstraction of symbolically
described states and transition function.

\section{Symbolic Abstraction}

A different approach to abstract a set of states is to use symbolic methods --
symbolic execution~\cite{King76, Cadar2013symbolic, Baldoni2018survey}, symbolic model
checking~\cite{Clarke96, McMillan93, Cimatti20}, bounded model
checking~\cite{Biere2003bounded} or control-explicit data-symbolic model
checking (\ceds)~\cite{Mrazek2016}.

In between symbolic execution tools, a prominent position represents
\textsc{klee}~\cite{Cadar2008klee}. It is a symbolic executor based on \llvm,
which enjoys widespread adoption and a lot of tools are built on top of
it~\cite{Beyer2018Klee, Chalupa2018, Chen2018, Menezes2018}. This is mainly because it
has a robust engine for system verification and support of \llvm as input
language.

Similarly, as in abstract interpretation, symbolic methods employ refinement
techniques~\cite{Beyer2016}. As many of these techniques are primarily based on
\smt theories~\cite{Beyer2018}, the symbolic verification may apply abstraction
strategies from solvers -- for example a bitwidth
abstractions~\cite{Jonavs2018} or theory refinements~\cite{Hyvarinen2017}.

As in the case of abstraction with uninterpreted functions~\cite{Gange2016}, \smt is also used to
a better approximation of abstract operations via symbolic abstraction~\cite{Thakur2012}.

\add{ extend symbolic abstraction technique }

\section{Interpolation}
\label{sec:interpolation}

Another technique used to improve abstraction-based techniques is an
interpolation. The core idea of the interpolation is to take feasible and
infeasible abstract paths, where the latter subsumes the first one, and refine
the feasible path to abstract more concrete paths but remaining still feasible
(i.e., subsumed by the infeasible state)~\cite{Mcmillan2003interpolation}.  The
interpolation is usually a part of \cegar loop as a refinement procedure for
spurious counterexamples. It has wide variety of use cases: in generation of
relevant predicates for predicate abstraction~\cite{Henzinger2004abstractions,
Cimatti2016}; range predicates in array abstraction~\cite{Jhala2007array} or
as refinement for symbolic~\cite{Ibing2016} and concolic execution~\cite{Jaffar2013}.

The interpolation was also applied in non-symbolic abstractions -- in the tool
DAGGER~\cite{Gulavani2008}, the interpolation is used to improve the precision
of widening of octagon and polyhedra domains to avoid false alarms. Similarly,
in VINTA~\cite{Albarghouthi2012}, the interpolation is used to refine the interval domain.

\section{Static analysis and transformation}

A static analysis often uses code transformation and instrumentation to
simplify its duties. A common technique is to reduce code size either directly
by a compiler~\cite{Namjoshi2018} or by program slicing~\cite{Chalupa2018}.

Another possibility is to use static analyzers to generate program invariants into the code for dynamic analysis~\cite{Gurfinkel2015} or instrument dynamic checks for memory safety analysis~\cite{Chalupa2019}.
Memory analysis also leverages instrumentation to keep numeric properties of heap-based data structures (e.g., the height of the tree, maximal element in the linked-list)~\cite{Magill2010}.

The closest approach to what we will describe in the following chapters is
concolic testing presented in CUTE~\cite{Sen2005}. CUTE instruments concolic
execution into the program, but it is restricted just to symbolic abstraction
in compare to our generic approach.
